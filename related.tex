%auto-ignore
\section{Related Work}
\subsection{Image Generation Models} 
Variational autoencoders \citep{vqvae} and Generative Adversarial Models (GANs) have shown excellent image generation performance with many variants proposed for both convolutional and Transformer architectures e.g. \citep{goodfellow2020generative,esser2021taming,karras2019style,brock2018large,donahue2019large}. Until recently, GANs were considered state of the art. Diffusion models, based on progressive denoising principles, are now able to synthesize images and video at equal or higher fidelity \citep{ddpm,kingma2021variational,ho2022video}. Hybrid approaches that combine principles from multiple approaches have also shown excellent performance \citep{maskgit,lezama2022improved}, suggesting that there are more complementarities between approaches that can be exploited.

\subsection{Image Tokenizers}
Image tokenizers are proving to be useful for multiple generative models due to the ability to move the bulk of the computation from input (pixel) space to latents \citep{ldm}, or to enabling more effective loss functions such as classification instead of regression \citep{maskgit,lezama2022improved,li2022mage}. A number of tokenization approaches such as Discrete VAE's \citep{rolfe2016discrete}, VQVAE \citep{vqvae} and VQGAN \citep{esser2021taming} have been developed, with the latter being the highest-performing as it combines perceptual and adversarial losses to achieve excellent reconstruction. ViT-VQGAN \citep{yu2021vector} extends VQGAN to the Transformer architecture. We use VQGAN rather than ViT-VQGAN as we found it to perform better for our model, noting that a better performing tokenization model does not always translate to a better performing text-to-image model.

\subsection{Large Language Models}
Our work leverages T5, a pre-trained large language model (LLM) that has been trained on multiple text-to-text tasks \citep{t5xxl}. LLMs (including T5, BERT \citep{bert}, and GPT \citep{brown2020language,radford2019language}) have been shown to learn powerful embeddings which enable few-shot transfer learning. We leverage this capacity in our model. All of the modern LLMs are trained on token prediction tasks (either autoregressive or not). The insights regarding the power of token prediction is leveraged in this work, where we apply a transformer to predict \emph{visual} tokens.

\subsection{Text-Image Models}
Leveraging paired text-image data is proving to be a powerful learning paradigm for representation learning and generative models. CLIP \citep{clip} and ALIGN \citep{jia2021scaling} train models to align pairs of text and image embeddings, showing excellent transfer and few-shot capabilities. Imagen \citep{imagen} and Parti \citep{parti} use similar large scale text-image datasets \citep{laion,schuhmann2022laion} to learn how to predict images from text inputs, achieving excellent results on FID and human evaluations. A key trick is the use of classifier-free guidance \citep{ho2022classifier,dhariwal2021diffusion} that trades off diversity and quality.

\subsection{Image Editing with Generative Models}
GANs have been extensively studied for image editing and manipulation capabilities (see \citep{xia2022gan} for a survey). A number of techniques have been developed on diffusion models to enable editing, personalization and inversion to token space \citep{gal2022image, meng2021sdedit, dreambooth, imagic, brooks2022instructpix2pix, prompttoprompt, nulltext2022}. Dreambooth \citep{dreambooth} and Imagic \citep{imagic} involve fine-tuning of the generative models. ImagenEditor \citep{imageneditor} frames the editing task as text-guided image inpainting, and involves user specified masks.
