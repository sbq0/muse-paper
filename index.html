<span 
class="ptmr7t-">oddsidemargin has been altered.</span><br 
class="newline" />
<span 
class="ptmbi7t-x-x-120">The page layout violates the ICML style.</span>
<span 
class="ptmr7t-">Please do not change the page layout, or include packages</span>
<span 
class="ptmr7t-">like geometry, savetrees, or fullpage, which change it for</span>
<span 
class="ptmr7t-">you.</span>
<span 
class="ptmr7t-">We&#8217;re not able to reliably undo arbitrary changes to the style.</span>
<span 
class="ptmr7t-">Please remove the offending package(s), or layout-changing</span>
<span 
class="ptmr7t-">commands and try again.</span>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="main.tex"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
                                                                                            
                                                                                             
_____________________________________________________________________________________________
<!--l. 162--><p class="noindent" ><span 
class="ptmb7t-x-x-144">Muse: Text-To-Image Generation via Masked Generative Transformers</span>______________
<div class="center" 
>
<!--l. 182--><p class="noindent" >
<!--l. 183--><p class="noindent" ><span 
class="ptmb7t-">Huiwen Chang</span> <sup class="textsuperscript"><span 
class="ptmr7t-x-x-90">*</span> </sup>  <span 
class="ptmb7t-">Han Zhang</span> <sup class="textsuperscript"><span 
class="ptmr7t-x-x-90">*</span> </sup>  <span 
class="ptmb7t-">Jarred Barber</span> <sup class="textsuperscript"><span 
class="ptmr8c-x-x-90">&#8224;</span> </sup>  <span 
class="ptmb7t-">AJ Maschinot</span> <sup class="textsuperscript"><span 
class="ptmr8c-x-x-90">&#8224;</span> </sup>  <span 
class="ptmb7t-">Jos</span><span 
class="ptmb7t-">é Lezama</span>   <span 
class="ptmb7t-">Lu Jiang</span>   <span 
class="ptmb7t-">Ming-Hsuan Yang</span>
<span 
class="ptmb7t-">Kevin Murphy</span>   <span 
class="ptmb7t-">William T. Freeman</span>   <span 
class="ptmb7t-">Michael Rubinstein</span> <sup class="textsuperscript"><span 
class="ptmr8c-x-x-90">&#8224;</span> </sup>  <span 
class="ptmb7t-">Yuanzhen Li</span> <sup class="textsuperscript"><span 
class="ptmr8c-x-x-90">&#8224;</span> </sup>  <span 
class="ptmb7t-">Dilip Krishnan</span> <sup class="textsuperscript"><span 
class="ptmr8c-x-x-90">&#8224;</span> </sup> </div>
<div class="center" 
>
<!--l. 197--><p class="noindent" >
<!--l. 198--><p class="noindent" ><span 
class="ptmr7t-x-x-120">Google Research</span></div>
<!--l. 222--><p class="noindent" ><a 
 id="x1-2f0"></a><span class="footnote-mark"><a 
 id="fn0x0">     <sup class="textsuperscript"></sup></a></span><sup class="textsuperscript"><span 
class="ptmr7t-x-x-90">*</span> </sup><span 
class="ptmr7t-x-x-90">Equal contribution</span> <sup><span 
class="cmsy-6">&#8224;</span></sup><span 
class="ptmr7t-x-x-90">Core contribution. Correspondence to: Huiwen Chang </span><span 
class="cmmi-9">&#x003C;</span><span 
class="ptmr7t-x-x-90">huiwenchang@google.com</span><span 
class="cmmi-9">&#x003E;</span><span 
class="ptmr7t-x-x-90">, Han Zhang </span><span 
class="cmmi-9">&#x003C;</span><span 
class="ptmr7t-x-x-90">zhanghan@google.com</span><span 
class="cmmi-9">&#x003E;</span><span 
class="ptmr7t-x-x-90">,</span>
<span 
class="ptmr7t-x-x-90">Dilip Krishnan </span><span 
class="cmmi-9">&#x003C;</span><span 
class="ptmr7t-x-x-90">dilipkay@google.com</span><span 
class="cmmi-9">&#x003E;</span><span 
class="ptmr7t-x-x-90">.</span>
<!--l. 222--><p class="indent" >    <span 
class="ptmr7t-x-x-90">&#x00A0;</span><br 
class="newline" />
<div 
class="abstract" 
>
<div  
class="centerline">                                                                                                  <span 
class="ptmb7t-x-x-120">Abstract</span>                                                                          </div>
       <div class="quote">
       <!--l. 3--><p class="noindent" >We   present   Muse,   a   text-to-image   Transformer   model   that   achieves   state-of-the-art   image   generation
        performance while being significantly more efficient than diffusion or autoregressive models. Muse&#x00A0;is trained
        on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large
        language model (LLM), Muse&#x00A0;is trained to predict randomly masked image tokens. Compared to pixel-space
        diffusion models, such as Imagen and DALL-E&#x00A0;2, Muse&#x00A0;is significantly more efficient due to the use of discrete
        tokens  and  requiring  fewer  sampling  iterations;  compared  to  autoregressive  models,  such  as  Parti,  Muse&#x00A0;is
        more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language
        understanding, translating to high-fidelity image generation and the understanding of visual concepts such as
        objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on
        CC3M, with an FID score of 6.06. The Muse&#x00A0;3B parameter model achieves an FID of 7.88&#x00A0;on zero-shot COCO
        evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications
        without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are
        available at <a 
href="http://\website" class="url" ><span 
class="pcrr7t-">http://' website</span></a>.
        </div>
</div>
<a 
 id="x1-3r1"></a>
<span 
class="ptmb7t-x-x-120">1.</span><span 
class="ptmb7t-x-x-120">&#x00A0;</span><span 
class="ptmb7t-x-x-120">Introduction</span>
<a 
 id="Q1-1-1"></a>                                                                                             
                                                                                             
Generative image models conditioned on text prompts have taken an enormous leap in quality and flexibility in
the last few years (<a 
href="#Xdalle2">Ramesh et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdalle2">2022</a>;&#x00A0;<a 
href="#Xglide">Nichol et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xglide">2021</a>;&#x00A0;<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>;&#x00A0;<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>;&#x00A0;<a 
href="#Xldm">Rombach
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>;&#x00A0;<a 
href="#Xmidjourney">Midjourney</a>,&#x00A0;<a 
href="#Xmidjourney">2022</a>). This was enabled by a combination of deep learning architecture innovations (<a 
href="#Xvqvae">Van
Den&#x00A0;Oord et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xvqvae">2017</a>;&#x00A0;<a 
href="#Xvaswani2017attention">Vaswani et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xvaswani2017attention">2017</a>); novel training paradigms such as masked modeling for both language
(<a 
href="#Xbert">Devlin et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xbert">2018</a>;&#x00A0;<a 
href="#Xt5xxl">Raffel et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xt5xxl">2020</a>) and vision tasks (<a 
href="#XMAE">He et&#x00A0;al.</a>,&#x00A0;<a 
href="#XMAE">2022</a>;&#x00A0;<a 
href="#Xmaskgit">Chang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmaskgit">2022</a>); new families of
generative models such as diffusion (<a 
href="#Xddpm">Ho et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xddpm">2020</a>;&#x00A0;<a 
href="#Xldm">Rombach et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>;&#x00A0;<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>) and masking-based
generation (<a 
href="#Xmaskgit">Chang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmaskgit">2022</a>); and finally, the availability of large scale image-text paired datasets (<a 
href="#Xlaion">Schuhmann
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xlaion">2021</a>).
                                                                                             
                                                                                             
<a 
 id="x1-4r1"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
 <img 
src="figs/teaser_v1-.png" alt="PIC"  
width="487" height="487" ><a 
 id="x1-5"></a>
<br /> <div class="caption" 
><span class="id">Figure 1: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Muse</span><span 
class="ptmr7t-x-x-90">&#x00A0;text-to-image generation (</span><span 
class="cmr-9">512 </span><span 
class="cmsy-9">&#x00D7; </span><span 
class="cmr-9">512 </span><span 
class="ptmr7t-x-x-90">resolution). Under each generated image, the corresponding caption is shown,</span>
<span 
class="ptmr7t-x-x-90">exhibiting a variety of styles, captions and understanding. Each image was generated in </span><span 
class="cmr-9">1</span><span 
class="cmmi-9">.</span><span 
class="cmr-9">3</span><span 
class="ptmr7t-x-x-90">s on a TPUv4 chip. </span></span></div><!--tex4ht:label?: x1-4r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
                                                                                             
                                                                                             
<a 
 id="x1-6r2"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
 <img 
src="figs/teaser_editing_1-.png" alt="PIC"  
width="487" height="487" ><a 
 id="x1-7"></a>
<br /> <div class="caption" 
><span class="id">Figure 2: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Examples of zero-shot text-guided image editing using Muse. We show examples of a number of editing applications using</span>
<span 
class="ptmr7t-x-x-90">the Muse</span><span 
class="ptmr7t-x-x-90">&#x00A0;text-to-image generative model, on </span><span 
class="ptmri7t-x-x-90">real </span><span 
class="ptmr7t-x-x-90">input images, without fine-tuning. All edited images are generated at </span><span 
class="cmr-9">512 </span><span 
class="cmsy-9">&#x00D7; </span><span 
class="cmr-9">512</span>
<span 
class="ptmr7t-x-x-90">resolution. </span></span></div><!--tex4ht:label?: x1-6r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
In this work, we present a new model for text-to-image synthesis using a masked image modeling approach (<a 
href="#Xmaskgit">Chang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmaskgit">2022</a>).
Our image decoder architecture is conditioned on embeddings from a pre-trained and frozen T5-XXL (<a 
href="#Xt5xxl">Raffel et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xt5xxl">2020</a>) large
language model (LLM) encoder. In agreement with Imagen (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>), we find that conditioning on a pre-trained LLM
is crucial for photorealistic, high quality image generation. Our models (except for the VQGAN quantizer) are built on the
Transformer (<a 
href="#Xvaswani2017attention">Vaswani et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xvaswani2017attention">2017</a>) architecture.
We have trained a sequence of Muse&#x00A0;models, ranging in size from 632M parameters to 3B parameters (for the image
decoder; the T5-XXL model has an additional 4.6B parameters). Each model consists of several sub-models (Figure <a 
href="#x1-11r3">3<!--tex4ht:ref: fig:model --></a>):
First, we have a pair of VQGAN &#8220;tokenizer&#8221; models (<a 
href="#Xesser2021taming">Esser et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xesser2021taming">2021b</a>), which can encode an input image to a
sequence of discrete tokens as well as decode a token sequence back to an image. We use two VQGANs, one for
256x256 resolution (&#8220;low-res&#8221;) and another for 512x512 resolution (&#8220;high-res&#8221;). Second, we have a base masked
image model, which contains the bulk of our parameters. This model takes a sequence of partially masked low-res
tokens and predicts the marginal distribution for each masked token, conditioned on the unmasked tokens and a
T5XXL text embedding. Third, we have a &#8220;superres&#8221; transformer model which translates (unmasked) low-res tokens
into high-res tokens, again conditioned on T5-XXL text embeddings. We explain our pipeline in detail in Section
<a 
href="#x1-13r2">2<!--tex4ht:ref: sec:model --></a>.
Compared to Imagen (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>) or Dall-E2 (<a 
href="#Xdalle2">Ramesh et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdalle2">2022</a>) which are built on cascaded pixel-space diffusion
models, Muse&#x00A0;is significantly more efficient due to the use of discrete tokens; it can be thought of as a discrete diffusion process
with the absorbing state (<span 
class="pcrr7t-">[MASK]</span>)&#x00A0;(<a 
href="#Xaustin2021structured">Austin et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xaustin2021structured">2021</a>). Compared to Parti (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>), a state-of-the-art autoregressive
model, Muse&#x00A0;is more efficient due to the use of parallel decoding. Based on comparisons on similar hardware (TPU-v4
chips), we estimate that Muse&#x00A0;is more than <span 
class="cmr-10">10</span>x faster at inference time than either Imagen-3B or Parti-3B models
and <span 
class="cmr-10">3</span>x faster than Stable Diffusion v1.4 (<a 
href="#Xldm">Rombach et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>) (see Section <a 
href="#x1-46r2">3.2.2<!--tex4ht:ref: sec:speed --></a>). All these comparisons are
when images of the same size: either <span 
class="cmr-10">256 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">256 </span>or <span 
class="cmr-10">512 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">512</span>. Muse&#x00A0;is also faster than Stable Diffusion (<a 
href="#Xldm">Rombach
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>), in spite of both models working in the latent space of a VQGAN. We believe that this is due to the use of a
diffusion model in Stable Diffusion v1.4 which requires a significantly higher number of iterations at inference
time.
The efficiency improvement of Muse, however, does <span 
class="ptmri7t-">not </span>come at a loss of generated image quality or semantic understanding of the
input text prompt. We evaluate our output on multiple criteria, including CLIP score (<a 
href="#Xclip">Radford et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xclip">2021</a>) and FID (<a 
href="#Xfid">Heusel
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xfid">2017</a>). The former is a measure of image-text correspondence; and the latter a measure of image quality and diversity. Our
3B parameter model achieves a CLIP score of 0.32&#x00A0;and an FID score of 7.88&#x00A0;on the COCO (<a 
href="#Xcoco">Lin et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xcoco">2014</a>) zero-shot
validation benchmark, which compares favorably with that of other large-scale text-to-image models (see Table <a 
href="#x1-36r2">2<!--tex4ht:ref: tab:eval_coco --></a>). Our
632M(base)+268M(super-res) parameter model achieves a state of the art FID score of 6.06&#x00A0; when trained and evaluated on the
CC3M (<a 
href="#Xsharma2018conceptual">Sharma et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xsharma2018conceptual">2018</a>) dataset, which is significantly lower than all other reported results in the literature (see Table <a 
href="#x1-34r1">1<!--tex4ht:ref: tab:eval_cc3m --></a>). We
also evaluate our generations on the PartiPrompts (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>) evaluation suite with human raters, who find that
Muse&#x00A0;generates images better aligned with its text prompt <span 
class="cmr-10">2</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">7</span>x more often than Stable Diffusion v1.4 (<a 
href="#Xldm">Rombach
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>).
Muse&#x00A0;generates images that reflect different parts of speech in input captions, including nouns, verbs and adjectives. Furthermore,
we present evidence of multi-object properties understanding, such as compositionality and cardinality, as well image style
understanding. See Figure <a 
href="#x1-4r1">1<!--tex4ht:ref: fig:teaser_t2i --></a> for a number of these examples and our website <a 
href="http://\website" class="url" ><span 
class="pcrr7t-">http://' website</span></a> for more examples. The
mask-based training of Muse&#x00A0;lends itself to a number of zero-shot image editing capabilities. A number of these are shown in
Figure <a 
href="#x1-6r2">2<!--tex4ht:ref: fig:teaser_edit --></a>, including zero-shot, text-guided inpainting, outpainting and mask-free editing. More details are in Section <a 
href="#x1-27r3">3<!--tex4ht:ref: sec:results --></a>. Our
contributions are:
    <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
 1. </dt><dd 
class="enumerate-enumitem">We  present  a  state-of-the-art  model  for  text-to-image  generation  which  achieves  excellent  FID  and  CLIP  scores
    (quantitative measures of image generation quality, diversity and alignment with text prompts).
    </dd><dt class="enumerate-enumitem">
 2. </dt><dd 
class="enumerate-enumitem">Our  model  is  significantly  faster  than  comparable  models  due  to  the  use  of  quantized  image  tokens  and  parallel
    decoding.
    </dd><dt class="enumerate-enumitem">
 3. </dt><dd 
class="enumerate-enumitem">Our  architecture  enables  out-of-the-box,  zero-shot  editing  capabilities  including  inpainting,  outpainting,  and
                                                                                             
                                                                                             
    mask-free editing.</dd></dl>
                                                                                             
                                                                                             
<!--l. 1--><p class="noindent" ><a 
 id="x1-11r3"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="center" 
>
<!--l. 2--><p class="noindent" >
<!--l. 4--><p class="noindent" > <img 
src="figs/pipeline_v4-.png" alt="PIC"  
width="414" height="414" ></div>
<a 
 id="x1-12"></a>
<br /> <div class="caption" 
><span class="id">Figure 3: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Muse</span><span 
class="ptmr7t-x-x-90">&#x00A0;Framework: We show the training pipeline for our model, with the T5-XXL pre-trained text encoder, base model and</span>
<span 
class="ptmr7t-x-x-90">super-resolution model depicted on the three rows. The text encoder generates a text embedding that is used for cross-attention with</span>
<span 
class="ptmr7t-x-x-90">image tokens for both base and super-res Transformer layers. The base model uses a VQ Tokenizer that is pre-trained on lower resolution</span>
<span 
class="ptmr7t-x-x-90">(</span><span 
class="cmr-9">256 </span><span 
class="cmsy-9">&#x00D7; </span><span 
class="cmr-9">256</span><span 
class="ptmr7t-x-x-90">) images and generates a </span><span 
class="cmr-9">16 </span><span 
class="cmsy-9">&#x00D7; </span><span 
class="cmr-9">16 </span><span 
class="ptmr7t-x-x-90">latent space of tokens. This sequence is masked at a variable rate per sample and then the</span>
<span 
class="ptmr7t-x-x-90">cross-entropy loss learns to predict the masked image tokens. Once the base model is trained, the reconstructed lower-resolution tokens</span>
<span 
class="ptmr7t-x-x-90">and text tokens are passed into the super-res model that then learns to predict masked tokens at a higher resolution. </span></span></div><!--tex4ht:label?: x1-11r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
<a 
 id="x1-13r2"></a>
<span 
class="ptmb7t-x-x-120">2.</span><span 
class="ptmb7t-x-x-120">&#x00A0;</span><span 
class="ptmb7t-x-x-120">Model</span>
<a 
 id="Q1-1-2"></a>
Our model is built on a number of components. Here, we provide an overview of each of those components in the order of their
training, while relegating many details of the architecture and parameters to the Appendix. Figure <a 
href="#x1-11r3">3<!--tex4ht:ref: fig:model --></a> provides an overview of the
model architecture.
<a 
 id="x1-14r1"></a>
<span 
class="ptmb7t-">2.1.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Pre-trained</span>
    <span 
class="ptmb7t-">Text</span>
    <span 
class="ptmb7t-">Encoders</span>
<a 
 id="Q1-1-3"></a>
Similar to the findings in (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>), we find that leveraging a pre-trained large language model (LLM) is beneficial to
high-quality image generation. The embeddings extracted from an LLM such as T5-XXL (<a 
href="#Xt5xxl">Raffel et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xt5xxl">2020</a>) carry rich
information about objects (nouns), actions (verbs), visual properties (adjectives), spatial relationships (prepositions), and other
properties such as cardinality and composition. Our hypothesis is that the Muse&#x00A0;model learns to map these rich visual and semantic
concepts in the LLM embeddings to the generated images; it has been shown in recent work (<a 
href="#Xmerullo2022linearly">Merullo et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmerullo2022linearly">2022</a>) that the
conceptual representations learned by LLM&#8217;s are roughly linearly mappable to those learned by models trained on vision tasks.
Given an input text caption, we pass it through the frozen T5-XXL encoder, resulting in a sequence of 4096 dimensional language
embedding vectors. These embedding vectors are linearly projected to the hidden size of our Transformer models (base and
super-res).
<a 
 id="x1-15r2"></a>
<span 
class="ptmb7t-">2.2.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Semantic</span>
    <span 
class="ptmb7t-">Tokenization</span>
    <span 
class="ptmb7t-">using</span>
    <span 
class="ptmb7t-">VQGAN</span>
<a 
 id="Q1-1-4"></a>
A core component of our model is the use of semantic tokens obtained from a VQGAN (<a 
href="#Xesser2021taming">Esser et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xesser2021taming">2021b</a>) model. This model
consists of an encoder and an decoder, with a quantization layer that maps an input image into a sequence of tokens from a learned
codebook. We build our encoder and decoder entirely with convolutional layers to support encoding images from different
resolutions. The encoder has several downsampling blocks to reduce the spatial dimension of the input, while the decoder has the
corresponding number of upsampling blocks to map the latents back into original image size. Given an image of size <span 
class="cmmi-10">H </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmmi-10">W </span>,
the encoded token is of size <sup class="nicefrac"><span 
class="cmmi-10">H</span></sup><span 
class="cmmi-10">&#x2215;</span><sub class="nicefrac"><span 
class="cmmi-10">f</span></sub> <span 
class="cmsy-10">&#x00D7;</span><sup class="nicefrac"> <span 
class="cmmi-10">W</span></sup><span 
class="cmmi-10">&#x2215;</span><sub class="nicefrac"><span 
class="cmmi-10">f</span></sub>, with downsampling ratio <span 
class="cmmi-10">f</span>. We train two VQGAN models: one with
downsampling ratio <span 
class="cmmi-10">f </span><span 
class="cmr-10">= 16 </span>and the other with downsampling ratio <span 
class="cmmi-10">f </span><span 
class="cmr-10">= 8</span>. We obtain tokens for our base model
using the <span 
class="cmmi-10">f </span><span 
class="cmr-10">= 16 </span>VQGAN model on 256<span 
class="cmsy-10">&#x00D7;</span>256 pixel images, thus resulting in tokens with spatial size <span 
class="cmr-10">16 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">16</span>. We
obtain the tokens for our super-resolution model using the <span 
class="cmmi-10">f </span><span 
class="cmr-10">= 8 </span>VQGAN model on <span 
class="cmr-10">512 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">512 </span>images, and the
corresponding token has spatial size <span 
class="cmr-10">64 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">64</span>. As mentioned in previous work (<a 
href="#Xesser2021taming">Esser et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xesser2021taming">2021b</a>), the resulting discrete
tokens after encoding capture higher-level semantics of the image, while ignoring low level noise. Furthermore, the
discrete nature of these tokens allows us to use a cross-entropy loss at the output to predict masked tokens in the next
stage.
<a 
 id="x1-16r3"></a>
<span 
class="ptmb7t-">2.3.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Base</span>
    <span 
class="ptmb7t-">Model</span>
<a 
 id="Q1-1-5"></a>
Our base model is a masked transformer(<a 
href="#Xvaswani2017attention">Vaswani et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xvaswani2017attention">2017</a>;&#x00A0;<a 
href="#Xbert">Devlin et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xbert">2018</a>), where the inputs are the projected T5
embeddings and image tokens. We leave all the text embeddings unmasked and randomly mask a varying fraction of image tokens
(see Section <a 
href="#x1-21r6">2.6<!--tex4ht:ref: sec:masking --></a>) and replace them with a special <span 
class="pcrr7t-">[MASK]</span>token (<a 
href="#Xmaskgit">Chang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmaskgit">2022</a>). We then linearly map image tokens into
                                                                                             
                                                                                             
image input embeddings of the required Transformer input/hidden size along with learned 2D positional embeddings. Following
previous transformer architecture (<a 
href="#Xvaswani2017attention">Vaswani et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xvaswani2017attention">2017</a>), we use several transformer layers including self-attention block,
cross-attention block and MLP block to extract features. At the output layer, an MLP is used to convert each masked image
embedding to a set of logits (corresponding to the VQGAN codebook size) and a cross-entropy loss is applied with the ground truth
token label as the target. At training, the base model is trained to predict all masked tokens at each step. However, for
inference, mask prediction is performed in an iterative manner which significantly increases quality. See Section <a 
href="#x1-24r8">2.8<!--tex4ht:ref: sec:iterativedec --></a> for
details.
<a 
 id="x1-17r4"></a>
<span 
class="ptmb7t-">2.4.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Super-Resolution</span>
    <span 
class="ptmb7t-">Model</span>
<a 
 id="Q1-1-6"></a>
                                                                                             
                                                                                             
<a 
 id="x1-18r4"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="minipage"><!--l. 6--><p class="noindent" > <img 
src="figs/superres_arch_fig-.png" alt="PIC"  
width="307" height="307" >
</div>                               <div class="minipage"> <img 
src="figs/superres_examples-.png" alt="PIC"  
width="165" height="165" >
</div>
<a 
 id="x1-19"></a>
 <div class="caption" 
><span class="id">Figure 4: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Super-resolution Model. On the left is shown the architecture of the super-resolution model. Low-resolution tokens are</span>
<span 
class="ptmr7t-x-x-90">passed into a series of self-attention Transformer layers; and the resulting output embeddings are concatenated with text embeddings</span>
<span 
class="ptmr7t-x-x-90">extracted from the conditioning text prompt. Following this, cross-attention is applied from these concatenated embeddings to the masked</span>
<span 
class="ptmr7t-x-x-90">high-resolution tokens; the loss learns to predict these masked tokens conditioned on the low-resolution and text tokens. On the right are</span>
<span 
class="ptmr7t-x-x-90">shown two examples of the improvement brought about by the super-resolution model.</span></span></div><!--tex4ht:label?: x1-18r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
We found that directly predicting <span 
class="cmr-10">512 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">512 </span>resolution leads the model to focus on low-level details over large-scale semantics. As
a result we found it beneficial to use a cascade of models: first a base model that generates a <span 
class="cmr-10">16 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">16 </span>latent map
(corresponding to a <span 
class="cmr-10">256 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">256 </span>image), followed by a super-resolution model that upsamples the base latent map to a <span 
class="cmr-10">64 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">64</span>
latent map (corresponding to a <span 
class="cmr-10">512 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">512 </span>image). The super-res model is trained after the base model has been
trained.
As mentioned in Section <a 
href="#x1-15r2">2.2<!--tex4ht:ref: sec:vqgan --></a>, we trained two VQGAN models, one at <span 
class="cmr-10">16 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">16 </span>latent resolution and <span 
class="cmr-10">256 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">256 </span>spatial resolution, and
the second at <span 
class="cmr-10">64 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">64 </span>latent resolution and <span 
class="cmr-10">512 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">512 </span>spatial resolution. Since our base model outputs tokens corresponding to a
<span 
class="cmr-10">16 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">16 </span>latent map, our super-resolution procedure learns to &#8220;translate&#8221; the lower-resolution latent map to the higher-resolution
latent map, followed by decoding through the higher-resolution VQGAN to give the final high-resolution image. This latent map
translation model is also trained with text conditioning and cross-attention in an analogous manner to the base model, as shown in
Figure <a 
href="#x1-18r4">4<!--tex4ht:ref: fig:sr --></a>.
<a 
 id="x1-20r5"></a>
<span 
class="ptmb7t-">2.5.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Decoder</span>
    <span 
class="ptmb7t-">Finetuning</span>
<a 
 id="Q1-1-7"></a>
To further improve our model&#8217;s ability to generate fine details, we increase the capacity of the VQGAN decoder by the addition of
more residual layers and channels while keeping the encoder capacity fixed. We then finetune the new decoder layers while keeping
the VQGAN encoder weights, codebook and transformers (i.e., base model and super resolution model) frozen. This allows
us to improve our visual quality without re-training any of the other model components (because the visual token
&#8220;language&#8221; stays fixed). This is shown in Figure <a 
href="#x1-2008r13">13<!--tex4ht:ref: fig:finetune_decoder --></a> in the Appendix, where we see that the finetuned decoder can
reconstruct more sharper details in the store front. We also give details of the finetuned decoder architecture in the
Appendix.
<a 
 id="x1-21r6"></a>
<span 
class="ptmb7t-">2.6.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Variable</span>
    <span 
class="ptmb7t-">Masking</span>
    <span 
class="ptmb7t-">Rate</span>
<a 
 id="Q1-1-8"></a>
As was done in (<a 
href="#Xmaskgit">Chang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmaskgit">2022</a>), we train our model with a variable masking rate based on a Cosine scheduling: for each
training example, we sample a masking rate <span 
class="cmmi-10">r </span><span 
class="cmsy-10">&#x2208; </span><span 
class="cmr-10">[0</span><span 
class="cmmi-10">,</span><span 
class="cmr-10">1] </span>from a truncated <span 
class="cmr-10">arccos</span> distribution with density function
<span 
class="cmmi-10">p</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">r</span><span 
class="cmr-10">) =</span> <img 
src="main0x.png" alt="2
&#x03C0;"  class="frac" align="middle"><span 
class="cmr-10">(1 </span><span 
class="cmsy-10">-</span><span 
class="cmmi-10">r</span><sup><span 
class="cmr-7">2</span></sup><span 
class="cmr-10">)</span><sup><span 
class="cmsy-7">-</span><img 
src="main1x.png" alt="1
2"  class="frac" align="middle"></sup>. This has an expected masking rate of 0.64, with a strong bias towards higher masking rates. The bias towards
higher masking rates makes the prediction problem harder. In contrast with autoregressive approaches, which learn conditional
distributions <span 
class="cmmi-10">P</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">x</span><sub><span 
class="cmmi-7">i</span></sub><span 
class="cmsy-10">|</span><span 
class="cmmi-10">x</span><sub><span 
class="cmmi-7">&#x003C;i</span></sub><span 
class="cmr-10">) </span>for some fixed ordering of tokens, random masking with a variable masking ratio allows our models to
learn <span 
class="cmmi-10">P</span><span 
class="cmr-10">(</span><span 
class="cmmi-10">x</span><sub><span 
class="cmmi-7">i</span></sub><span 
class="cmsy-10">|</span><span 
class="cmmi-10">x</span><sub><span 
class="cmr-7">&#x039B;</span></sub><span 
class="cmr-10">) </span>for arbitrary subsets of tokens <span 
class="cmr-10">&#x039B;</span>. This is not only critical for our parallel sampling scheme, but
it also enables a number of zero-shot, out-of-the-box editing capabilities, such as shown in Figure <a 
href="#x1-6r2">2<!--tex4ht:ref: fig:teaser_edit --></a> and Section
<a 
href="#x1-47r3">3.3<!--tex4ht:ref: sec:editing --></a>.
<a 
 id="x1-22r7"></a>
<span 
class="ptmb7t-">2.7.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Classifier</span>
    <span 
class="ptmb7t-">Free</span>
    <span 
class="ptmb7t-">Guidance</span>
<a 
 id="Q1-1-9"></a>
We employ classifier-free guidance (CFG) (<a 
href="#Xho2022classifier">Ho &amp; Salimans</a>,&#x00A0;<a 
href="#Xho2022classifier">2022</a>) to improve our generation quality and our text-image alignment.
At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token
self-attention). At inference time, we compute a conditional logit <span 
class="cmmi-10">&#x2113;</span><sub><span 
class="cmmi-7">c</span></sub> and an unconditional logit <span 
class="cmmi-10">&#x2113;</span><sub><span 
class="cmmi-7">u</span></sub> for each masked
token. We then form the final logits <span 
class="cmmi-10">&#x2113;</span><sub><span 
class="cmmi-7">g</span></sub> by moving away from the unconditional logits by an amount <span 
class="cmmi-10">t</span>, the <span 
class="ptmri7t-">guidance</span>
<span 
class="ptmri7t-">scale</span>:
<table 
class="equation"><tr><td> <a 
 id="x1-23r1"></a>
                                                                                             
                                                                                             
<center class="math-display" >
<img 
src="main2x.png" alt="&#x2113; = (1+ t)&#x2113; - t&#x2113;
 g         c   u
" class="math-display" ></center></td><td class="equation-label">(1)</td></tr></table>
Intuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly
increasing the guidance scale <span 
class="cmmi-10">t </span>through the sampling procedure. This allows the early tokens to be sampled more freely, with low or
no guidance, but increases the influence of the conditioning prompt for the later tokens.
We also exploit this mechanism to enable <span 
class="ptmri7t-">negative prompting </span>(<a 
href="#Xnegprompt">NegPrompt</a>,&#x00A0;<a 
href="#Xnegprompt">2022</a>) by replacing the unconditional logit <span 
class="cmmi-10">&#x2113;</span><sub><span 
class="cmmi-7">u</span></sub> with a
logit conditioned on a &#8220;negative prompt&#8221;. This encourages the resulting image to have features associated with the positive prompt
<span 
class="cmmi-10">&#x2113;</span><sub><span 
class="cmmi-7">c</span></sub> and remove features associated with the negative prompt <span 
class="cmmi-10">&#x2113;</span><sub><span 
class="cmmi-7">u</span></sub>.
<a 
 id="x1-24r8"></a>
<span 
class="ptmb7t-">2.8.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Iterative</span>
    <span 
class="ptmb7t-">Parallel</span>
    <span 
class="ptmb7t-">Decoding</span>
    <span 
class="ptmb7t-">at</span>
    <span 
class="ptmb7t-">Inference</span>
<a 
 id="Q1-1-10"></a>
                                                                                             
                                                                                             
<a 
 id="x1-25r5"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="center" 
>
<!--l. 2--><p class="noindent" >
<!--l. 4--><p class="noindent" > <img 
src="figs/inference_fig-.png" alt="PIC"  
width="487" height="487" ></div>
<a 
 id="x1-26"></a>
<br /> <div class="caption" 
><span class="id">Figure 5: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Inference samples. We visualize the evolution of masked tokens over the sequence of steps for the base model (left) and the</span>
<span 
class="ptmr7t-x-x-90">super-res model (right). The super-res model, being conditioned on the low-res tokens, requires significantly fewer sampling steps for</span>
<span 
class="ptmr7t-x-x-90">convergence. </span></span></div><!--tex4ht:label?: x1-25r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
The critical component for our model&#8217;s inference time efficiency is the use of parallel decoding to predict multiple output tokens in a
single forward pass. The key assumption underlying the effectiveness of the parallel decoding is a Markovian property that many
tokens are conditionally independent given other tokens. Decoding is performed based on a cosine schedule (<a 
href="#Xmaskgit">Chang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmaskgit">2022</a>)
that chooses a certain fixed fraction of the highest confidence masked tokens that are to be predicted at that step.
These tokens are then set to unmasked for the remainder of the steps and the set of masked tokens is appropriately
reduced. Using this procedure, we are able to perform inference of <span 
class="cmr-10">256 </span>tokens using only <span 
class="cmr-10">24 </span>decoding steps in our base
model and <span 
class="cmr-10">4096 </span>tokens using <span 
class="cmr-10">8 </span>decoding steps in our super-resolution model, as compared to the 256 or 4096 steps
required for autoregressive models (e.g. (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>)) and hundreds of steps for diffusion models (e.g., (<a 
href="#Xldm">Rombach
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>;&#x00A0;<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>)). We note that recent methods including progressive distillation (<a 
href="#Xsalimans2022distillation">Salimans &amp;
Ho</a>,&#x00A0;<a 
href="#Xsalimans2022distillation">2022</a>) and better ODE solvers (<a 
href="#XZhu2022dpm">Lu et&#x00A0;al.</a>,&#x00A0;<a 
href="#XZhu2022dpm">2022</a>) have greatly reduced the sampling steps of diffusion models,
but they have not been widely validated in large scale text-to-image generation. We leave the comparison to these
faster methods in the future work, while noting that similar distillation approaches are also a possibility for our
model.
<a 
 id="x1-27r3"></a>
<span 
class="ptmb7t-x-x-120">3.</span><span 
class="ptmb7t-x-x-120">&#x00A0;</span><span 
class="ptmb7t-x-x-120">Results</span>
<a 
 id="Q1-1-11"></a>
We train a number of base Transformer models at different parameter sizes, ranging from 600M to 3B parameters. Each of these
models is fed in the output embeddings from a T5-XXL model, which is pre-trained and frozen and consists of 4.6B
parameters. Our largest base model of 3B parameters consists of <span 
class="cmr-10">48 </span>Transformer layers with cross-attention from text to
image and self-attention among image tokens. All base models share the same image tokenizer. We use a CNN
model with <span 
class="cmr-10">19 </span>ResNet blocks and a quantized codebook of size <span 
class="cmr-10">8192 </span>for the tokenization. Larger codebook sizes
did not result in performance improvements. The super-resolution model consists of <span 
class="cmr-10">32 </span>multi-axis Transformer
layers&#x00A0;(<a 
href="#Xhit">Zhao et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xhit">2021</a>) with cross-attention from concatenated text and image embedding to high resolution image and
self-attention among high resolution image tokens. This model converts a sequence of tokens from one latent space to
another: the first latent space being that of the base model tokenizer, a latent space of <span 
class="cmr-10">16 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">16 </span>tokens, to that of
a higher resolution tokenizer with <span 
class="cmr-10">64 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">64 </span>tokens. After token conversion, the decoder for the higher resolution
tokenizer is used to convert to the higher resolution image space. Further details of configurations are provided in the
appendix.
We train on the Imagen dataset consisting of 460M text-image pairs (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>). Training is performed for 1M steps, with
a batch size of 512 on 512-core TPU-v4 chips (<a 
href="#Xjouppi2020domain">Jouppi et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xjouppi2020domain">2020</a>). This takes about 1 week of training time. We use the Adafactor
optimizer (<a 
href="#Xshazeer2018adafactor">Shazeer &amp; Stern</a>,&#x00A0;<a 
href="#Xshazeer2018adafactor">2018</a>) to save on memory consumption which allowed us to fit a 3B parameter model without model
parallelization. We also avoid performing exponential moving averaging (EMA) of model weights during training, again to
save on TPU memory. In order to reap the benefits of EMA, we checkpoint every 5000 steps, then perform EMA
offline on the checkpointed weights with a decay factor of 0.7. These averaged weights form the final base model
weights.
<hr class="figure"><div class="figure" 
>
                                                                                             
                                                                                             
<a 
 id="x1-28r6"></a>
                                                                                             
                                                                                             
<div class="tabular"> <table id="TBL-1" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1"><col 
id="TBL-1-2"><col 
id="TBL-1-3"></colgroup><colgroup id="TBL-1-4g"><col 
id="TBL-1-4"><col 
id="TBL-1-5"><col 
id="TBL-1-6"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td colspan="3" style="white-space:normal; text-align:left;" id="TBL-1-1-1"  
class="td11">                   <div class="multicolumn"  style="white-space:nowrap; text-align:center;">Cardinality</div>                      </td><td colspan="3" style="white-space:normal; text-align:left;" id="TBL-1-1-4"  
class="td11">                  <div class="multicolumn"  style="white-space:nowrap; text-align:center;">Composition</div>
</td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-2-"><td  style="white-space:normal; text-align:left;" id="TBL-1-2-1"  
class="td11"> <!--l. 16--><p class="noindent" ><img 
src="figs/verticals/cardinality_00430_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-1-2-2"  
class="td11"> <!--l. 16--><p class="noindent" ><img 
src="figs/verticals/cardinality_00708_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-1-2-3"  
class="td11"> <!--l. 17--><p class="noindent" ><img 
src="figs/verticals/cardinality_00087_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-1-2-4"  
class="td11"> <!--l. 18--><p class="noindent" ><img 
src="figs/verticals/composition_00678_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-1-2-5"  
class="td11"> <!--l. 19--><p class="noindent" ><img 
src="figs/verticals/composition_00681_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-1-2-6"  
class="td11"> <!--l. 20--><p class="noindent" ><img 
src="figs/verticals/composition_01426_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-3-"><td  style="white-space:normal; text-align:left;" id="TBL-1-3-1"  
class="td11"> <!--l. 22--><p class="noindent" >
<span 
class="ptmr7t-x-x-60">Three  elephants  standing  on</span>
    <span 
class="ptmr7t-x-x-60">top of each other.</span>                        </td><td  style="white-space:normal; text-align:left;" id="TBL-1-3-2"  
class="td11"> <!--l. 23--><p class="noindent" ><span 
class="ptmr7t-x-x-60">Four wine bottles.</span>                      </td><td  style="white-space:normal; text-align:left;" id="TBL-1-3-3"  
class="td11"> <!--l. 24--><p class="noindent" ><span 
class="ptmr7t-x-x-60">A  tiny  football  in  front  of</span>
    <span 
class="ptmr7t-x-x-60">three yellow tennis balls.</span>            </td><td  style="white-space:normal; text-align:left;" id="TBL-1-3-4"  
class="td11"> <!--l. 25--><p class="noindent" ><span 
class="ptmr7t-x-x-60">Three small yellow boxes on</span>
    <span 
class="ptmr7t-x-x-60">a large blue box.</span>                         </td><td  style="white-space:normal; text-align:left;" id="TBL-1-3-5"  
class="td11"> <!--l. 26--><p class="noindent" ><span 
class="ptmr7t-x-x-60">A             large             present</span>
    <span 
class="ptmr7t-x-x-60">with a red ribbon to the left of</span>
    <span 
class="ptmr7t-x-x-60">a Christmas tree.</span>                        </td><td  style="white-space:normal; text-align:left;" id="TBL-1-3-6"  
class="td11"> <!--l. 27--><p class="noindent" ><span 
class="ptmr7t-x-x-60">Two  baseballs  to  the  left  of</span>
    <span 
class="ptmr7t-x-x-60">three tennis balls.</span>                       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-1-4-"><td  style="white-space:normal; text-align:left;" id="TBL-1-4-1"  
class="td11"> </td></tr><tr><td colspan="6"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-01-4-"><td  style="white-space:normal; text-align:left;" id="TBL-01-4-1"  
class="td11">
 <!--l. 29--><p class="noindent" >             </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-01-5-"><td  style="white-space:normal; text-align:left;" id="TBL-01-5-1"  
class="td11"> <!--l. 29--><p class="noindent" >             </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-01-6-"><td colspan="3" style="white-space:normal; text-align:left;" id="TBL-01-6-1"  
class="td11">       <div class="multicolumn"  style="white-space:nowrap; text-align:center;">Style</div>                        </td><td colspan="3" style="white-space:normal; text-align:left;" id="TBL-01-6-4"  
class="td11">                  <div class="multicolumn"  style="white-space:nowrap; text-align:center;">Text Rendering</div>
</td></tr><tr  
 style="vertical-align:baseline;" id="TBL-01-7-"><td  style="white-space:normal; text-align:left;" id="TBL-01-7-1"  
class="td11"> <!--l. 33--><p class="noindent" ><img 
src="figs/verticals/style_00020_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-01-7-2"  
class="td11"> <!--l. 34--><p class="noindent" ><img 
src="figs/verticals/style_00022_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-01-7-3"  
class="td11"> <!--l. 35--><p class="noindent" ><img 
src="figs/verticals/style_01342_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-01-7-4"  
class="td11"> <!--l. 36--><p class="noindent" ><img 
src="figs/verticals/text_00716_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-01-7-5"  
class="td11"> <!--l. 37--><p class="noindent" ><img 
src="figs/verticals/text_01470_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-01-7-6"  
class="td11"> <!--l. 38--><p class="noindent" ><img 
src="figs/verticals/text_01494_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-01-8-"><td  style="white-space:normal; text-align:left;" id="TBL-01-8-1"  
class="td11"> <!--l. 40--><p class="noindent" >
<span 
class="ptmr7t-x-x-60">Portrait   of   a   well-dressed</span>
    <span 
class="ptmr7t-x-x-60">raccoon,  oil  painting  in  the</span>
    <span 
class="ptmr7t-x-x-60">style of Rembrandt.</span>                    </td><td  style="white-space:normal; text-align:left;" id="TBL-01-8-2"  
class="td11"> <!--l. 41--><p class="noindent" ><span 
class="ptmr7t-x-x-60">A  portrait  of  a  man  wearing</span>
    <span 
class="ptmr7t-x-x-60">sunglasses   and   a   business</span>
    <span 
class="ptmr7t-x-x-60">suit, painting in pop art style.</span>     </td><td  style="white-space:normal; text-align:left;" id="TBL-01-8-3"  
class="td11"> <!--l. 42--><p class="noindent" ><span 
class="ptmr7t-x-x-60">Portrait   of   a   tiger   wearing</span>
    <span 
class="ptmr7t-x-x-60">a  train  conductor&#8217;s  hat  and</span>
    <span 
class="ptmr7t-x-x-60">holding   a   skateboard   that</span>
    <span 
class="ptmr7t-x-x-60">has   a   yin-yang   symbol   on</span>
    <span 
class="ptmr7t-x-x-60">it.   Chinese   ink   and   wash</span>
    <span 
class="ptmr7t-x-x-60">painting.</span>                                     </td><td  style="white-space:normal; text-align:left;" id="TBL-01-8-4"  
class="td11"> <!--l. 43--><p class="noindent" ><span 
class="ptmr7t-x-x-60">A  t-shirt  with  Carpe  Diem</span>
    <span 
class="ptmr7t-x-x-60">written on it.</span>                               </td><td  style="white-space:normal; text-align:left;" id="TBL-01-8-5"  
class="td11"> <!--l. 44--><p class="noindent" ><span 
class="ptmr7t-x-x-60">High-contrast  image  of  the</span>
    <span 
class="ptmr7t-x-x-60">word   &#8220;WOMBAT&#8221;   written</span>
    <span 
class="ptmr7t-x-x-60">with   thick   colored   graffiti</span>
    <span 
class="ptmr7t-x-x-60">letters  on  a  white  wall  with</span>
    <span 
class="ptmr7t-x-x-60">dramatic splashes of paint.</span>         </td><td  style="white-space:normal; text-align:left;" id="TBL-01-8-6"  
class="td11"> <!--l. 45--><p class="noindent" ><span 
class="ptmr7t-x-x-60">The</span>
    <span 
class="ptmr7t-x-x-60">saying   &#8220;BE   EXCELLENT</span>
    <span 
class="ptmr7t-x-x-60">TO  EACH  OTHER&#8221;  written</span>
    <span 
class="ptmr7t-x-x-60">in a stained glass window.</span>          </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-01-9-"><td  style="white-space:normal; text-align:left;" id="TBL-01-9-1"  
class="td11"> </td></tr><tr><td colspan="6"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-001-9-"><td  style="white-space:normal; text-align:left;" id="TBL-001-9-1"  
class="td11">
 <!--l. 47--><p class="noindent" >             </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-001-10-"><td  style="white-space:normal; text-align:left;" id="TBL-001-10-1"  
class="td11"> <!--l. 47--><p class="noindent" >             </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-001-11-"><td colspan="3" style="white-space:normal; text-align:left;" id="TBL-001-11-1"  
class="td11">               <div class="multicolumn"  style="white-space:nowrap; text-align:center;">Usage of Entire Prompt</div>                 </td><td colspan="3" style="white-space:normal; text-align:left;" id="TBL-001-11-4"  
class="td11">                <div class="multicolumn"  style="white-space:nowrap; text-align:center;">Failure Text Classes</div>
</td></tr><tr  
 style="vertical-align:baseline;" id="TBL-001-12-"><td  style="white-space:normal; text-align:left;" id="TBL-001-12-1"  
class="td11"> <!--l. 51--><p class="noindent" ><img 
src="figs/verticals/detail_00030_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-001-12-2"  
class="td11"> <!--l. 52--><p class="noindent" ><img 
src="figs/verticals/detail_01370_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-001-12-3"  
class="td11"> <!--l. 53--><p class="noindent" ><img 
src="figs/verticals/detail_01459_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-001-12-4"  
class="td11"> <!--l. 54--><p class="noindent" ><img 
src="figs/failures/failure_00036_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-001-12-5"  
class="td11"> <!--l. 55--><p class="noindent" ><img 
src="figs/failures/failure_00709_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td><td  style="white-space:normal; text-align:left;" id="TBL-001-12-6"  
class="td11"> <!--l. 56--><p class="noindent" ><img 
src="figs/failures/failure_00060_maskgit_sresg1r1.jpg" alt="PIC"  
width="73" height="73" >                         </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-001-13-"><td  style="white-space:normal; text-align:left;" id="TBL-001-13-1"  
class="td11"> <!--l. 58--><p class="noindent" ><span 
class="ptmr7t-x-x-60">An</span>
    <span 
class="ptmr7t-x-x-60">art  gallery  displaying  Monet</span>
    <span 
class="ptmr7t-x-x-60">paintings.  The  art  gallery  is</span>
    <span 
class="ptmr7t-x-x-60">flooded.   Robots   are   going</span>
    <span 
class="ptmr7t-x-x-60">around  the  art  gallery  using</span>
    <span 
class="ptmr7t-x-x-60">paddle boards.</span>                            </td><td  style="white-space:normal; text-align:left;" id="TBL-001-13-2"  
class="td11"> <!--l. 59--><p class="noindent" ><span 
class="ptmr7t-x-x-60">A  photograph  of  the  inside</span>
    <span 
class="ptmr7t-x-x-60">of  a  subway  train.  There  are</span>
    <span 
class="ptmr7t-x-x-60">raccoons sitting on the seats.</span>
    <span 
class="ptmr7t-x-x-60">One                                     of</span>
    <span 
class="ptmr7t-x-x-60">them is reading a newspaper.</span>
    <span 
class="ptmr7t-x-x-60">The window shows the city in</span>
    <span 
class="ptmr7t-x-x-60">the background.</span>                          </td><td  style="white-space:normal; text-align:left;" id="TBL-001-13-3"  
class="td11"> <!--l. 60--><p class="noindent" ><span 
class="ptmr7t-x-x-60">Two cups of coffee, one with</span>
    <span 
class="ptmr7t-x-x-60">latte art of yin yang symbol.</span>
    <span 
class="ptmr7t-x-x-60">The  other  has  latter  art  of  a</span>
    <span 
class="ptmr7t-x-x-60">heart.</span>                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-001-13-4"  
class="td11"> <!--l. 61--><p class="noindent" ><span 
class="ptmr7t-x-x-60">A cartoon of a dog saying &#8220;I</span>
    <span 
class="ptmr7t-x-x-60">see what you did there&#8221;.</span>             </td><td  style="white-space:normal; text-align:left;" id="TBL-001-13-5"  
class="td11"> <!--l. 62--><p class="noindent" ><span 
class="ptmr7t-x-x-60">Ten wine bottles.</span>                        </td><td  style="white-space:normal; text-align:left;" id="TBL-001-13-6"  
class="td11"> <!--l. 63--><p class="noindent" ><span 
class="ptmr7t-x-x-60">A basketball game between a</span>
    <span 
class="ptmr7t-x-x-60">team of four cats and a team</span>
    <span 
class="ptmr7t-x-x-60">of three dogs.</span>                              </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-001-14-"><td  style="white-space:normal; text-align:left;" id="TBL-001-14-1"  
class="td11">              </td></tr></table></div>
<a 
 id="x1-29"></a>
<br /> <div class="caption" 
><span class="id">Figure  6:  </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Examples  demonstrating  text-to-image  capabilities  of  Muse</span><span 
class="ptmr7t-x-x-90">&#x00A0;for  various  text  properties.  Top  left:  cardinality;  top  right:</span>
<span 
class="ptmr7t-x-x-90">composition; middle left: style; middle right: text rendering; and bottom left: usage of the entire prompt. For all examples, </span><span 
class="cmr-9">16 </span><span 
class="ptmr7t-x-x-90">instances</span>
<span 
class="ptmr7t-x-x-90">per prompt were generated, and the one with the highest CLIP score (</span><a 
href="#Xclip"><span 
class="ptmr7t-x-x-90">Radford et</span><span 
class="ptmr7t-x-x-90">&#x00A0;al.</span></a><span 
class="ptmr7t-x-x-90">,</span><span 
class="ptmr7t-x-x-90">&#x00A0;</span><a 
href="#Xclip"><span 
class="ptmr7t-x-x-90">2021</span></a><span 
class="ptmr7t-x-x-90">) was chosen. Bottom right: examples of</span>
<span 
class="ptmr7t-x-x-90">generated image failure in Muse</span><span 
class="ptmr7t-x-x-90">&#x00A0;for various text properties such as direct rendering of long phrases, high cardinalities, and multiple</span>
<span 
class="ptmr7t-x-x-90">cardinalities.</span></span></div><!--tex4ht:label?: x1-28r -->
                                                                                             
                                                                                             
</div><hr class="endfigure">
                                                                                             
                                                                                             
<a 
 id="x1-30r7"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<img 
src="figs/comparison_muse_watermark.jpg" alt="PIC"  
width="453" height="453" >
<a 
 id="x1-31"></a>
<br /> <div class="caption" 
><span class="id">Figure 7: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Comparing the same prompts across DALL-E2 (</span><a 
href="#Xdalle2"><span 
class="ptmr7t-x-x-90">Ramesh et</span><span 
class="ptmr7t-x-x-90">&#x00A0;al.</span></a><span 
class="ptmr7t-x-x-90">,</span><span 
class="ptmr7t-x-x-90">&#x00A0;</span><a 
href="#Xdalle2"><span 
class="ptmr7t-x-x-90">2022</span></a><span 
class="ptmr7t-x-x-90">) (left), Imagen (</span><a 
href="#Ximagen"><span 
class="ptmr7t-x-x-90">Saharia et</span><span 
class="ptmr7t-x-x-90">&#x00A0;al.</span></a><span 
class="ptmr7t-x-x-90">,</span><span 
class="ptmr7t-x-x-90">&#x00A0;</span><a 
href="#Ximagen"><span 
class="ptmr7t-x-x-90">2022</span></a><span 
class="ptmr7t-x-x-90">) (middle) and</span>
<span 
class="ptmr7t-x-x-90">Muse</span><span 
class="ptmr7t-x-x-90">&#x00A0;(right). </span></span></div><!--tex4ht:label?: x1-30r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
<a 
 id="x1-32r1"></a>
<span 
class="ptmb7t-">3.1.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Qualitative</span>
    <span 
class="ptmb7t-">Performance</span>
<a 
 id="Q1-1-12"></a>
Figure <a 
href="#x1-28r6">6<!--tex4ht:ref: fig:text_class_examples --></a> qualitatively demonstrates the capabilities of Muse&#x00A0;for text prompts with different properties. The top left of <a 
href="#x1-28r6">Figure&#x00A0;6</a>
shows examples that demonstrate a basic understanding of cardinality. For objects with non-unity cardinality, instead of
generating the same object pixels multiple times, Muse&#x00A0;instead adds contextual variations to make the overall image
more realistic, e.g., elephant size and orientation, wine bottle wrapper color, and tennis ball rotation. The top right
of Fig, <a 
href="#x1-28r6">6<!--tex4ht:ref: fig:text_class_examples --></a> demonstrates understanding of multi-object composition and relativeness. Instead of placing objects at
random locations, Muse&#x00A0;generates images that preserve prepositional object relations in the text, e.g., on vs under, left
vs right, etc. The middle left of <a 
href="#x1-28r6">Figure&#x00A0;6</a> demonstrates its ability to generate images spanning many styles, both
specific to a renowned artist (e.g., Rembrandt) as well as general to a style as a whole (e.g., pop art and Chinese
ink and wash). The middle right of <a 
href="#x1-28r6">Figure&#x00A0;6</a> demonstrates the ability of Muse&#x00A0;to render words and phrases. Text
generation is fundamentally different than generating most other objects. Instead of the model learning a mapping
between an object name and its characteristics (e.g., that &#8220;elephant&#8221; maps to &#8220;large&#8221;, &#8220;gray&#8221;, and &#8220;peanut eating&#8221;), the
virtual continuum of possible words and phrases demands that the model learn differently. It must instead learn a
hierarchical understanding between phrases, words, and letters. The bottom left of <a 
href="#x1-28r6">Figure&#x00A0;6</a> demonstrates that Muse&#x00A0;uses
the entirety of a text prompt when rendering instead of focusing exclusively on only a few salient words. Finally,
<a 
href="#x1-30r7">Figure&#x00A0;7</a> shows comparisons between Muse, Dall-E 2 (<a 
href="#Xdalle2">Ramesh et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdalle2">2022</a>), and Imagen (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>)
for some select prompts, showing that Muse&#x00A0;is at par with Imagen and qualitatively better than Dall-E2 for many
prompts.
However, as demonstrated in the bottom right of <a 
href="#x1-28r6">Figure&#x00A0;6</a>, Muse&#x00A0;is limited in its ability to generate images well aligned with
certain types of prompts. For prompts which indicate that long, multi-word phrases should be directly rendered,
&#x00A0;Muse&#x00A0;has a tendency to render those phrases incorrectly, often resulting in (unwanted) duplicated rendered words or
rendering of only a portion of the phrase. Additionally, prompts indicating high object cardinality tend to result in
generated images which do not correctly reflect that desired cardinality (e.g., rendering only <span 
class="cmr-10">7 </span>wine bottles when
the prompt specified <span 
class="cmr-10">10</span>). In general, the ability of Muse&#x00A0;to render the correct cardinalities of objects decreases as
the cardinality increases. Another difficult prompt type for Muse&#x00A0;is ones with multiple cardinalities (e.g., &#8220;four
cats and a team of three dogs&#8221;). For such cases, Muse&#x00A0;has a tendency to get at least one cardinality incorrect in its
rendering.
<a 
 id="x1-33r2"></a>
<span 
class="ptmb7t-">3.2.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Quantitative</span>
    <span 
class="ptmb7t-">Performance</span>
<a 
 id="Q1-1-13"></a>
<div class="table">
                                                                                             
                                                                                             
<a 
 id="x1-34r1"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="center" 
>
<!--l. 5--><p class="noindent" >
<div class="tabular">
 <table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"></colgroup><colgroup id="TBL-2-2g"><col 
id="TBL-2-2"></colgroup><colgroup id="TBL-2-3g"><col 
id="TBL-2-3"></colgroup><colgroup id="TBL-2-4g"><col 
id="TBL-2-4"><col 
id="TBL-2-5"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:normal; text-align:left;" id="TBL-2-1-1"  
class="td11"> <!--l. 8--><p class="noindent" ><span 
class="ptmb7t-">Approach</span>                                          </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-2"  
class="td11">     <span 
class="ptmb7t-">Model Type           </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-1-3"  
class="td11">     <span 
class="ptmb7t-">Params  </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-1-4"  
class="td11">  <span 
class="ptmb7t-">FID  </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-1-5"  
class="td11"> <span 
class="ptmb7t-">CLIP  </span></td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:normal; text-align:left;" id="TBL-2-2-1"  
class="td11"> <!--l. 14--><p class="noindent" >VQGAN&#x00A0;(<a 
href="#Xesser2021taming">Esser et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xesser2021taming">2021b</a>)          </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-2"  
class="td11">    Autoregressive          </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-2-3"  
class="td11">       600M  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-2-4"  
class="td11"> 28.86  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-2-5"  
class="td11">  0.20  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:normal; text-align:left;" id="TBL-2-3-1"  
class="td11"> <!--l. 15--><p class="noindent" >ImageBART&#x00A0;(<a 
href="#Xesser2021imagebart">Esser et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xesser2021imagebart">2021a</a>)    </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-2"  
class="td11"> Diffusion+Autogressive  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-3-3"  
class="td11">        2.8B  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-3-4"  
class="td11"> 22.61  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-3-5"  
class="td11">  0.23  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:normal; text-align:left;" id="TBL-2-4-1"  
class="td11"> <!--l. 16--><p class="noindent" >LDM-4&#x00A0;(<a 
href="#Xldm">Rombach et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>)       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-2"  
class="td11">      Diffusion              </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-4-3"  
class="td11">       645M  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-4-4"  
class="td11"> 17.01  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-4-5"  
class="td11">  0.24  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:normal; text-align:left;" id="TBL-2-5-1"  
class="td11"> <!--l. 17--><p class="noindent" >RQ-Transformer                       (<a 
href="#Xlee2022autoregressive">Lee
  et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xlee2022autoregressive">2022a</a>)                                     </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-2"  
class="td11">    Autoregressive          </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-5-3"  
class="td11">       654M  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-5-4"  
class="td11"> 12.33  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-5-5"  
class="td11">  0.26  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-6-"><td  style="white-space:normal; text-align:left;" id="TBL-2-6-1"  
class="td11"> <!--l. 18--><p class="noindent" >Draft-and-revise                       (<a 
href="#Xlee2022draft">Lee
  et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xlee2022draft">2022b</a>)                                     </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-2"  
class="td11">   Non-autoregressive      </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-6-3"  
class="td11">       654M  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-6-4"  
class="td11">  9.65  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-6-5"  
class="td11">  0.26  </td>

</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-7-"><td  style="white-space:normal; text-align:left;" id="TBL-2-7-1"  
class="td11"> <!--l. 22--><p class="noindent" ><span 
class="ptmb7t-">Muse(base model)</span>                             </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-2"  
class="td11">   Non-autoregressive      </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-7-3"  
class="td11">       632M  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-7-4"  
class="td11">   6.8  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-7-5"  
class="td11">  0.25  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-8-"><td  style="white-space:normal; text-align:left;" id="TBL-2-8-1"  
class="td11"> <!--l. 24--><p class="noindent" ><span 
class="ptmb7t-">Muse(base + super-res)</span>                     </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-8-2"  
class="td11">   Non-autoregressive      </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-8-3"  
class="td11"> 632M + 268M  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-8-4"  
class="td11">  6.06  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-2-8-5"  
class="td11">  0.26  </td>

</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-9-"><td  style="white-space:normal; text-align:left;" id="TBL-2-9-1"  
class="td11">                          </td></tr></table></div></div>
<a 
 id="x1-35"></a>
<br /><div class="caption" 
><span class="id">Table 1: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Quantitative evaluation on CC3M (</span><a 
href="#Xsharma2018conceptual"><span 
class="ptmr7t-x-x-90">Sharma et</span><span 
class="ptmr7t-x-x-90">&#x00A0;al.</span></a><span 
class="ptmr7t-x-x-90">,</span><span 
class="ptmr7t-x-x-90">&#x00A0;</span><a 
href="#Xsharma2018conceptual"><span 
class="ptmr7t-x-x-90">2018</span></a><span 
class="ptmr7t-x-x-90">); all models are trained and evaluated on CC3M.</span></span></div><!--tex4ht:label?: x1-34r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
</div>
<div class="table">
                                                                                             
                                                                                             
<a 
 id="x1-36r2"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="tabular">
 <table id="TBL-3" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-3-1g"><col 
id="TBL-3-1"></colgroup><colgroup id="TBL-3-2g"><col 
id="TBL-3-2"></colgroup><colgroup id="TBL-3-3g"><col 
id="TBL-3-3"></colgroup><colgroup id="TBL-3-4g"><col 
id="TBL-3-4"><col 
id="TBL-3-5"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-3-1-"><td  style="white-space:normal; text-align:left;" id="TBL-3-1-1"  
class="td11"> <!--l. 11--><p class="noindent" ><div class="multirow"><!-- rows=86422285 -->
<span 
class="ptmb7t-">Approach</span></div>                                          </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-1-2"  
class="td11">   <div class="multirow"><!-- rows=86422285 -->
<span 
class="ptmb7t-">Model Type</span></div>        </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-1-3"  
class="td11"> <div class="multirow"><!-- rows=86422285 -->
<span 
class="ptmb7t-">Params</span></div>  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-1-4"  
class="td11"> <div class="multirow"><!-- rows=86422285 -->
<span 
class="ptmb7t-">FID-30K</span></div>  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-1-5"  
class="td11"> <span 
class="ptmb7t-">Zero-shot  </span></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-2-"><td  style="white-space:normal; text-align:left;" id="TBL-3-2-1"  
class="td11"> <!--l. 15--><p class="noindent" > </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-2-2"  
class="td11"> </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-2-3"  
class="td11"> </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-2-4"  
class="td11"> </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-2-5"  
class="td11"> <span 
class="ptmb7t-">FID-30K</span></td>

</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-3-"><td  style="white-space:normal; text-align:left;" id="TBL-3-3-1"  
class="td11"> <!--l. 17--><p class="noindent" >AttnGAN (<a 
href="#Xattngan">Xu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xattngan">2017</a>)               </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-3-2"  
class="td11">      GAN              </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-3-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-3-4"  
class="td11">    35.49  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-3-5"  
class="td11">       -  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-4-"><td  style="white-space:normal; text-align:left;" id="TBL-3-4-1"  
class="td11"> <!--l. 18--><p class="noindent" >DM-GAN (<a 
href="#Xzhu2019dm">Zhu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xzhu2019dm">2019</a>)             </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-4-2"  
class="td11">      GAN              </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-4-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-4-4"  
class="td11">    32.64  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-4-5"  
class="td11">       -  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-5-"><td  style="white-space:normal; text-align:left;" id="TBL-3-5-1"  
class="td11"> <!--l. 19--><p class="noindent" >DF-GAN (<a 
href="#Xdfgan">Tao et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdfgan">2020</a>)               </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-5-2"  
class="td11">      GAN              </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-5-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-5-4"  
class="td11">    21.42  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-5-5"  
class="td11">       -  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-6-"><td  style="white-space:normal; text-align:left;" id="TBL-3-6-1"  
class="td11"> <!--l. 20--><p class="noindent" >DM-GAN + CL (<a 
href="#Xdmgan-cl">Ye et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdmgan-cl">2021</a>)      </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-6-2"  
class="td11">      GAN              </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-6-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-6-4"  
class="td11">    20.79  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-6-5"  
class="td11">       -  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-7-"><td  style="white-space:normal; text-align:left;" id="TBL-3-7-1"  
class="td11"> <!--l. 21--><p class="noindent" >XMC-GAN (<a 
href="#Xzhang2021cross">Zhang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xzhang2021cross">2021</a>)       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-7-2"  
class="td11">      GAN              </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-7-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-7-4"  
class="td11">    9.33  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-7-5"  
class="td11">       -  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-8-"><td  style="white-space:normal; text-align:left;" id="TBL-3-8-1"  
class="td11"> <!--l. 22--><p class="noindent" >LAFITE (<a 
href="#Xlafite">Zhou et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xlafite">2021</a>)              </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-8-2"  
class="td11">      GAN              </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-8-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-8-4"  
class="td11">    8.12  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-8-5"  
class="td11">       -  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-9-"><td  style="white-space:normal; text-align:left;" id="TBL-3-9-1"  
class="td11"> <!--l. 23--><p class="noindent" >Make-A-Scene                      (<a 
href="#Xmakeascene">Gafni
  et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmakeascene">2022</a>)                                       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-9-2"  
class="td11">   Autoregressive       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-9-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-9-4"  
class="td11">    7.55  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-9-5"  
class="td11">       -  </td>

</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-10-"><td  style="white-space:normal; text-align:left;" id="TBL-3-10-1"  
class="td11"> <!--l. 25--><p class="noindent" >DALL-E (<a 
href="#Xdalle1">Ramesh et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdalle1">2021</a>)         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-10-2"  
class="td11">   Autoregressive       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-10-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-10-4"  
class="td11">       -  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-10-5"  
class="td11">    17.89  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-11-"><td  style="white-space:normal; text-align:left;" id="TBL-3-11-1"  
class="td11"> <!--l. 26--><p class="noindent" >LAFITE (<a 
href="#Xlafite">Zhou et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xlafite">2021</a>)              </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-11-2"  
class="td11">      GAN              </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-11-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-11-4"  
class="td11">       -  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-11-5"  
class="td11">    26.94  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-12-"><td  style="white-space:normal; text-align:left;" id="TBL-3-12-1"  
class="td11"> <!--l. 27--><p class="noindent" >LDM (<a 
href="#Xldm">Rombach et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>)            </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-12-2"  
class="td11">     Diffusion           </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-12-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-12-4"  
class="td11">       -  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-12-5"  
class="td11">    12.63  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-13-"><td  style="white-space:normal; text-align:left;" id="TBL-3-13-1"  
class="td11"> <!--l. 28--><p class="noindent" >GLIDE (<a 
href="#Xglide">Nichol et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xglide">2021</a>)             </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-13-2"  
class="td11">     Diffusion           </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-13-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-13-4"  
class="td11">       -  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-13-5"  
class="td11">    12.24  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-14-"><td  style="white-space:normal; text-align:left;" id="TBL-3-14-1"  
class="td11"> <!--l. 29--><p class="noindent" >DALL-E 2 (<a 
href="#Xdalle2">Ramesh et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdalle2">2022</a>)      </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-14-2"  
class="td11">     Diffusion           </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-14-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-14-4"  
class="td11">       -  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-14-5"  
class="td11">    10.39  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-15-"><td  style="white-space:normal; text-align:left;" id="TBL-3-15-1"  
class="td11"> <!--l. 30--><p class="noindent" >Imagen-3.4B                       (<a 
href="#Ximagen">Saharia
  et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>)                                       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-15-2"  
class="td11">     Diffusion           </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-15-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-15-4"  
class="td11">       -  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-15-5"  
class="td11">     7.27  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-16-"><td  style="white-space:normal; text-align:left;" id="TBL-3-16-1"  
class="td11"> <!--l. 32--><p class="noindent" >Parti-3B (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>)                  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-16-2"  
class="td11">   Autoregressive       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-16-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-16-4"  
class="td11">       -  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-16-5"  
class="td11">     8.10  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-17-"><td  style="white-space:normal; text-align:left;" id="TBL-3-17-1"  
class="td11"> <!--l. 33--><p class="noindent" >Parti-20B (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>)                </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-17-2"  
class="td11">   Autoregressive       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-17-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-17-4"  
class="td11">    3.22  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-17-5"  
class="td11">     7.23  </td>

</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-18-"><td  style="white-space:normal; text-align:left;" id="TBL-3-18-1"  
class="td11"> <!--l. 36--><p class="noindent" ><span 
class="ptmb7t-">Muse-3B</span>                                             </td><td  style="white-space:nowrap; text-align:center;" id="TBL-3-18-2"  
class="td11"> Non-Autoregressive  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-18-3"  
class="td11">       </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-18-4"  
class="td11">       -  </td><td  style="white-space:nowrap; text-align:right;" id="TBL-3-18-5"  
class="td11">     7.88  </td>

</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-19-"><td  style="white-space:normal; text-align:left;" id="TBL-3-19-1"  
class="td11">                          </td></tr></table></div>
<a 
 id="x1-37"></a>
<br /> <div class="caption" 
><span class="id">Table 2: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Quantitative evaluation of FID and CLIP score (where available) on MS-COCO (</span><a 
href="#Xcoco"><span 
class="ptmr7t-x-x-90">Lin et</span><span 
class="ptmr7t-x-x-90">&#x00A0;al.</span></a><span 
class="ptmr7t-x-x-90">,</span><span 
class="ptmr7t-x-x-90">&#x00A0;</span><a 
href="#Xcoco"><span 
class="ptmr7t-x-x-90">2014</span></a><span 
class="ptmr7t-x-x-90">) for </span><span 
class="cmr-9">256 </span><span 
class="cmsy-9">&#x00D7; </span><span 
class="cmr-9">256 </span><span 
class="ptmr7t-x-x-90">image</span>
<span 
class="ptmr7t-x-x-90">resolution. Muse</span><span 
class="ptmr7t-x-x-90">&#x00A0; achieves a CLIP score of 0.32, higher than the score of 0.27 reported in Imagen. Other papers in the table above did</span>
<span 
class="ptmr7t-x-x-90">not report a CLIP score.</span></span></div><!--tex4ht:label?: x1-36r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
</div>
<hr class="figure"><div class="figure" 
>
                                                                                             
                                                                                             
<a 
 id="x1-38r8"></a><a 
 id="x1-40r9"></a><a 
 id="x1-44r3"></a>
                                                                                             
                                                                                             
<div class="tabular"> <table id="TBL-4" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-4-1g"><col 
id="TBL-4-1"><col 
id="TBL-4-2"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-4-1-"><td  style="white-space:normal; text-align:left;" id="TBL-4-1-1"  
class="td11"> <!--l. 86--><p class="noindent" ><img 
src="figs/clip_fid-.png" alt="PIC"  
width="195" height="146" >
  <a 
 id="x1-39"></a>
<br />  <div class="caption" 
><span class="id">Figure  8:  </span><span  
class="content">CLIP  vs.  FID  tradeoff  curve.  We
perform  sweeps  of  sampling  parameters  for  a
fixed model, then plot the Pareto front.</span></div><!--tex4ht:label?: x1-38r -->
<!--l. 89--><p class="noindent" >                                                                                </td><td  style="white-space:normal; text-align:left;" id="TBL-4-1-2"  
class="td11"> <!--l. 91--><p class="noindent" ><img 
src="figs/rater_percentages_w_all_labels.jpg" alt="PIC"  
width="195" height="146" >
  <a 
 id="x1-41"></a>
<br />   <div class="caption" 
><span class="id">Figure   9:   </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Percentage   of   prompts   for   which   a</span>
<span 
class="ptmr7t-x-x-90">human   rater   consensus   chose   a   model   alignment</span>
<span 
class="ptmr7t-x-x-90">preference.  Contributions  from  specific  numbers  of</span>
<span 
class="ptmr7t-x-x-90">rater consensuses are shown in different colors, while</span>
<span 
class="ptmr7t-x-x-90">marginals over consensuses (</span><span 
class="cmr-9">= 5</span><span 
class="ptmr7t-x-x-90">, </span><span 
class="cmsy-9">&#x2265; </span><span 
class="cmr-9">4</span><span 
class="ptmr7t-x-x-90">, and </span><span 
class="cmsy-9">&#x2265; </span><span 
class="cmr-9">3</span><span 
class="ptmr7t-x-x-90">) are</span>
<span 
class="ptmr7t-x-x-90">shown numerically.</span></span></div><!--tex4ht:label?: x1-40r -->                                                          </td>
</tr></table></div>
                                                                                             
                                                                                             
</div><hr class="endfigure">
In Table <a 
href="#x1-34r1">1<!--tex4ht:ref: tab:eval_cc3m --></a> and Table <a 
href="#x1-36r2">2<!--tex4ht:ref: tab:eval_coco --></a>, we show our performance against other methods on the CC3M (<a 
href="#Xsharma2018conceptual">Sharma et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xsharma2018conceptual">2018</a>) and COCO (<a 
href="#Xcoco">Lin
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xcoco">2014</a>) datasets as measured by Fréchet Inception Distance (FID) (<a 
href="#Xfid">Heusel et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xfid">2017</a>), which measures quality and diversity
of samples, as well as CLIP (<a 
href="#Xclip">Radford et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xclip">2021</a>) score, which measures image/text alignment. For the CC3M results, both Muse&#x00A0;
models were trained on CC3M. The COCO results are zero-shot, using a model trained on the same dataset as Imagen (<a 
href="#Ximagen">Saharia
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>).
Our 632M model achieves SOTA results on CC3M, significantly improving upon the state of the art in FID score, and also achieving
state of the art CLIP score. Our 3B model achieves an FID score of 7.88&#x00A0;which is slightly better than the score of <span 
class="cmr-10">8</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">1 </span>achieved by
the Parti-3B model which has a similar number of parameters. Our CLIP score of 0.32&#x00A0;is higher than the CLIP score of 0.29
achieved by Imagen (which is achieved when the FID is significantly higher &#x00A0;20). For the FID of 7.27, Imagen achieves a CLIP
score of around 0.27 (see Figure 4 in (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>)).
Our sampling algorithm (<a 
href="#x1-24r8">Section&#x00A0;2.8</a>) has a number of hyperparameters, such as guidance scale, sampling temperature, whether
or not to linearly increase guidance during sampling, etc. We perform evaluation sweeps over these parameters.
We find subsets of sampling parameters that are Pareto efficient, in the sense that we cannot improve FID without
hurting CLIP. This allows us to study the tradeoff between diversity and image/text alignment, which we show in
<a 
href="#x1-38r8">Figure&#x00A0;8</a>.
<a 
 id="x1-42r1"></a>
<span 
class="ptmrc7t-">3.2.1.</span><span 
class="ptmrc7t-">&#x00A0;</span><span 
class="ptmrc7t-">H<span 
class="small-caps">u</span><span 
class="small-caps">m</span><span 
class="small-caps">a</span><span 
class="small-caps">n</span></span>
      <span 
class="ptmrc7t-"><span 
class="small-caps">e</span><span 
class="small-caps">v</span><span 
class="small-caps">a</span><span 
class="small-caps">l</span><span 
class="small-caps">u</span><span 
class="small-caps">a</span><span 
class="small-caps">t</span><span 
class="small-caps">i</span><span 
class="small-caps">o</span><span 
class="small-caps">n</span></span>
<a 
 id="Q1-1-14"></a>
Similar to previous works (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>;&#x00A0;<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>), we perform side-by-side evaluations in which human raters are
presented with a text prompt and two images, each generated by a different text-to-image model using that prompt. The raters are
asked to assess prompt-image alignment via the question, &#8220;Which image matches with the caption better?&#8221; Each image pair is
anonymized and randomly ordered (left vs right). Raters have the option of choosing either image or that they are
indifferent<span class="footnote-mark"><a 
href="main2.html#fn1x0"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-43f1"></a> .
Each (prompt, image pair) triplet is assessed by five independent raters; the raters were provided through the Google internal crowd
computing team and were completely anonymous to the Muse&#x00A0;team. For the set of prompts presented to raters, we used
PartiPrompts (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>), a collection of <span 
class="cmr-10">1650 </span>text prompts curated to measure model capabilities across a variety of
categories. For the two text-to-image models, we compared Muse&#x00A0;(<span 
class="cmr-10">3</span>B parameters) to that of Stable Diffusion v1.4 (<a 
href="#Xldm">Rombach
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>), the text-to-image model most comparable to Muse&#x00A0;in terms of inference speed. For each prompt, <span 
class="cmr-10">16 </span>image instances
were generated, and the one with the highest CLIP score (<a 
href="#Xclip">Radford et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xclip">2021</a>) was used. The stable diffusion images were
generated via the CompVis Stable Diffusion v1.4 notebook (<a 
href="#Xsdgeneration">CompVis</a>,&#x00A0;<a 
href="#Xsdgeneration">2022</a>). We required at least a <span 
class="cmr-10">3 </span>rater consensus for results to
be counted in favor of a particular model. From this analysis, we found that Muse&#x00A0;was chosen as better aligned than
Stable Diffusion for <span 
class="cmr-10">70</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">6</span>% of the prompts, Stable Diffusion was chosen as better aligned than Muse&#x00A0;for <span 
class="cmr-10">25</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">4</span>%, and
no rater consensus was chosen for <span 
class="cmr-10">4</span>%. These results are consistent with Muse&#x00A0;having significantly better caption
matching capability (<span 
class="cmsy-10">~ </span><span 
class="cmr-10">2</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">7</span>x). <a 
href="#x1-40r9">Figure&#x00A0;9</a> shows a breakdown of the rater results for rater consensuses of <span 
class="cmr-10">3</span>, <span 
class="cmr-10">4</span>, and all <span 
class="cmr-10">5</span>
possible votes. Prompts for which all <span 
class="cmr-10">5 </span>raters said Muse&#x00A0;had better alignment than Stable Diffusion are the larger
contributor.
In addition to measuring alignment, other works (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>;&#x00A0;<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>) have also measured image
realism, often via a rater question similar to, &#8220;Which image is more realistic?&#8221;. However, we note that care must be
taken with examination of such results. Though it is not the intent of the question, a model that is completely mode
collapsed so that it generates the same sufficiently realistic image regardless of prompt will virtually always do better
on this question than a model that <span 
class="ptmri7t-">does </span>take the prompt into account during image generation. We propose this
type of question is only applicable between models of similar alignment. Since Muse&#x00A0;is significantly better aligned
                                                                                             
                                                                                             
than Stable Diffusion, we did not assess realism via human raters. We consider this topic an area of open research.
<a 
 id="x1-46r2"></a>
<span 
class="ptmrc7t-">3.2.2.</span><span 
class="ptmrc7t-">&#x00A0;</span><span 
class="ptmrc7t-">I<span 
class="small-caps">n</span><span 
class="small-caps">f</span><span 
class="small-caps">e</span><span 
class="small-caps">r</span><span 
class="small-caps">e</span><span 
class="small-caps">n</span><span 
class="small-caps">c</span><span 
class="small-caps">e</span></span>
      <span 
class="ptmrc7t-">S<span 
class="small-caps">p</span><span 
class="small-caps">e</span><span 
class="small-caps">e</span><span 
class="small-caps">d</span></span>
<a 
 id="Q1-1-15"></a>
In <a 
href="#x1-44r3">Table&#x00A0;3</a>, we compare the inference time of Muse&#x00A0;to several other popular models. We benchmarked Parti-3B, Imagen, and
Muse-3B internally on TPUv4 accelerators. For Stable Diffusion/LDM, we used the fastest reported benchmark (<a 
href="#Xsdinference">Lambda
Labs</a>,&#x00A0;<a 
href="#Xsdinference">2022</a>), which was done on A100 GPUs. For Stable Diffusion, the TPU implementations we tested were not faster than the
A100 implementation. We also report an inference time for LDM with 250 iterations, which is the configuration used to achieve the
FID in <a 
href="#x1-36r2">Table&#x00A0;2</a>. Muse&#x00A0;is significantly faster than competing diffusion or autoregressive models, despite having comparable
parameter counts (and around 3x more parameters than Stable Diffusion/LDM). The speed advantage of Muse&#x00A0;over Imagen is due
to the use of discrete tokens and requiring fewer sampling iterations. The speed advantage of Muse&#x00A0;over Parti is due to the use of
parallel decoding. The speed advantage of Muse&#x00A0;over Stable Diffusion is primarily attributable to requiring fewer sampling
iterations.
<a 
 id="x1-47r3"></a>
<span 
class="ptmb7t-">3.3.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Image</span>
    <span 
class="ptmb7t-">Editing</span>
<a 
 id="Q1-1-16"></a>
By exploiting the fact that our model can condition on arbitrary subsets of image tokens, we can use the model out-of-the-box for a
variety of image editing applications with no additional training or model fine-tuning.
<div class="tabular"> <table id="TBL-5" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-5-1g"><col 
id="TBL-5-1"></colgroup><colgroup id="TBL-5-2g"><col 
id="TBL-5-2"></colgroup><colgroup id="TBL-5-3g"><col 
id="TBL-5-3"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-5-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-1-1"  
class="td11">    <span 
class="ptmb7t-">Model          </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-5-1-2"  
class="td11">  <span 
class="ptmb7t-">Resolution    </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-5-1-3"  
class="td11"> <span 
class="ptmb7t-">Time  </span></td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-2-1"  
class="td11"> Imagen </td><td  style="white-space:nowrap; text-align:center;" id="TBL-5-2-2"  
class="td11"> <span 
class="cmr-10">256 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">256 </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-5-2-3"  
class="td11"> 9.1s</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-5-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-3-1"  
class="td11">    Imagen          </td><td  style="white-space:nowrap; text-align:center;" id="TBL-5-3-2"  
class="td11"> <span 
class="cmr-10">1024 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">1024  </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-5-3-3"  
class="td11"> 13.3s  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-5-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-4-1"  
class="td11"> LDM (50 steps)   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-5-4-2"  
class="td11">  <span 
class="cmr-10">512 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">512   </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-5-4-3"  
class="td11">  3.7s  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-5-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-5-1"  
class="td11"> LDM (250 steps)  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-5-5-2"  
class="td11">  <span 
class="cmr-10">512 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">512   </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-5-5-3"  
class="td11"> 18.5s  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-5-6-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-6-1"  
class="td11">    Parti-3B         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-5-6-2"  
class="td11">  <span 
class="cmr-10">256 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">256   </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-5-6-3"  
class="td11">  6.4s  </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-5-7-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-7-1"  
class="td11">    Muse-3B         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-5-7-2"  
class="td11">  <span 
class="cmr-10">256 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">256   </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-5-7-3"  
class="td11">  0.5s  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-5-8-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-8-1"  
class="td11">    Muse-3B         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-5-8-2"  
class="td11">  <span 
class="cmr-10">512 </span><span 
class="cmsy-10">&#x00D7; </span><span 
class="cmr-10">512   </span></td><td  style="white-space:nowrap; text-align:right;" id="TBL-5-8-3"  
class="td11">  1.3s  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-5-9-"><td  style="white-space:nowrap; text-align:center;" id="TBL-5-9-1"  
class="td11">             </td></tr></table></div>
<a 
 id="x1-45"></a>
<br /> <div class="caption" 
><span class="id">Table  3:  </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Per-batch  inference  time  for  several  models.  Muse,</span>
<span 
class="ptmr7t-x-x-90">Imagen, and Parti were benchmarked internally on TPUv4 hardware.</span>
<span 
class="ptmr7t-x-x-90">Stable Diffusion/LDM benchmark from (</span><a 
href="#Xsdinference"><span 
class="ptmr7t-x-x-90">Lambda Labs</span></a><span 
class="ptmr7t-x-x-90">,</span><span 
class="ptmr7t-x-x-90">&#x00A0;</span><a 
href="#Xsdinference"><span 
class="ptmr7t-x-x-90">2022</span></a><span 
class="ptmr7t-x-x-90">), on</span>
<span 
class="ptmr7t-x-x-90">A100 GPUs. The &#8220;LDM (250 steps)&#8221; time comes from scaling the</span>
<span 
class="ptmr7t-x-x-90">50-step time by 5; 250 steps were used to achieve the FID in </span><a 
href="#x1-36r2"><span 
class="ptmr7t-x-x-90">Table</span><span 
class="ptmr7t-x-x-90">&#x00A0;</span><span 
class="ptmr7t-x-x-90">2</span></a><span 
class="ptmr7t-x-x-90">.</span></span></div><!--tex4ht:label?: x1-44r -->
                                                                                             
                                                                                             
<a 
 id="x1-48r10"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="tabular"> <table id="TBL-6" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-6-1g"><col 
id="TBL-6-1"><col 
id="TBL-6-2"><col 
id="TBL-6-3"><col 
id="TBL-6-4"><col 
id="TBL-6-5"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-6-1-"><td  style="white-space:normal; text-align:left;" id="TBL-6-1-1"  
class="td11"> <!--l. 4--><p class="noindent" ><img 
src="figs/inpaint/15_orig.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-6-1-2"  
class="td11"> <!--l. 5--><p class="noindent" ><img 
src="figs/inpaint/15_masked.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-6-1-3"  
class="td11"> <!--l. 6--><p class="noindent" ><img 
src="figs/inpaint/14_synth_07.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-6-1-4"  
class="td11"> <!--l. 7--><p class="noindent" ><img 
src="figs/inpaint/12_synth_00.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-6-1-5"  
class="td11"> <!--l. 8--><p class="noindent" ><img 
src="figs/inpaint/15_synth_07.jpg" alt="PIC"  
width="85" height="85" >                           </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-6-2-"><td  style="white-space:normal; text-align:left;" id="TBL-6-2-1"  
class="td11"> <!--l. 10--><p class="noindent" >Original </td><td  style="white-space:normal; text-align:left;" id="TBL-6-2-2"  
class="td11"> <!--l. 11--><p class="noindent" >Masked </td><td  style="white-space:normal; text-align:left;" id="TBL-6-2-3"  
class="td11"> <!--l. 12--><p class="noindent" >San Francisco in
  the background        </td><td  style="white-space:normal; text-align:left;" id="TBL-6-2-4"  
class="td11"> <!--l. 13--><p class="noindent" >New  York  City  in
  the background        </td><td  style="white-space:normal; text-align:left;" id="TBL-6-2-5"  
class="td11"> <!--l. 14--><p class="noindent" >Paris       in       the
  background              </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-6-3-"><td  style="white-space:normal; text-align:left;" id="TBL-6-3-1"  
class="td11"> <!--l. 16--><p class="noindent" ><img 
src="figs/inpaint/17_orig.jpg" alt="PIC"  
width="85" height="85" >                            </td><td  style="white-space:normal; text-align:left;" id="TBL-6-3-2"  
class="td11"> <!--l. 17--><p class="noindent" ><img 
src="figs/inpaint/18_masked.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-6-3-3"  
class="td11"> <!--l. 18--><p class="noindent" ><img 
src="figs/inpaint/18_synth_07.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-6-3-4"  
class="td11"> <!--l. 19--><p class="noindent" ><img 
src="figs/inpaint/old_pickup.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-6-3-5"  
class="td11"> <!--l. 20--><p class="noindent" ><img 
src="figs/inpaint/horse.jpg" alt="PIC"  
width="85" height="85" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-6-4-"><td  style="white-space:normal; text-align:left;" id="TBL-6-4-1"  
class="td11"> <!--l. 22--><p class="noindent" >Original                     </td><td  style="white-space:normal; text-align:left;" id="TBL-6-4-2"  
class="td11"> <!--l. 23--><p class="noindent" >Masked                     </td><td  style="white-space:normal; text-align:left;" id="TBL-6-4-3"  
class="td11"> <!--l. 24--><p class="noindent" >A    cabin    in    the
  woods                       </td><td  style="white-space:normal; text-align:left;" id="TBL-6-4-4"  
class="td11"> <!--l. 25--><p class="noindent" >An   old,   beat   up
  pickup truck.            </td><td  style="white-space:normal; text-align:left;" id="TBL-6-4-5"  
class="td11"> <!--l. 26--><p class="noindent" >A  horse  tied  to  a
  post.                         </td>
</tr></table></div>
<a 
 id="x1-49"></a>
<br /> <div class="caption" 
><span class="id">Figure 10: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Examples of text-guided inpainting. The mask is shown in the second column of each row. This behavior arises directly</span>
<span 
class="ptmr7t-x-x-90">from the model with no fine-tuning.</span></span></div><!--tex4ht:label?: x1-48r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
<a 
 id="x1-50r1"></a>
<span 
class="ptmrc7t-">3.3.1.</span><span 
class="ptmrc7t-">&#x00A0;</span><span 
class="ptmrc7t-">T<span 
class="small-caps">e</span><span 
class="small-caps">x</span><span 
class="small-caps">t</span>-<span 
class="small-caps">g</span><span 
class="small-caps">u</span><span 
class="small-caps">i</span><span 
class="small-caps">d</span><span 
class="small-caps">e</span><span 
class="small-caps">d</span></span>
      <span 
class="ptmrc7t-">I<span 
class="small-caps">n</span><span 
class="small-caps">p</span><span 
class="small-caps">a</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span><span 
class="small-caps">t</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span><span 
class="small-caps">g</span></span>
      <span 
class="ptmrc7t-">/</span>
      <span 
class="ptmrc7t-"><span 
class="small-caps">o</span><span 
class="small-caps">u</span><span 
class="small-caps">t</span><span 
class="small-caps">p</span><span 
class="small-caps">a</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span><span 
class="small-caps">t</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span><span 
class="small-caps">g</span></span>
<a 
 id="Q1-1-17"></a>
Our sampling procedure (<a 
href="#x1-24r8">Section&#x00A0;2.8</a>) gives us text-guided inpainting and outpainting for free: we convert an input image into a set
of tokens, mask out the tokens corresponding to a local region, and then sample the masked tokens conditioned on unmasked tokens
and a text prompt. We integrate superresolution through a multi-scale approach: Given an image of size 512x512, we first decimate
it to 256x256 and convert both images to high- and low-res tokens. Then, we mask out the appropriate regions for each set of tokens.
Next, we inpaint the low-res tokens using the parallel sampling algorithm. Finally, we condition on these low-res
tokens to inpaint the high-res tokens using the same sampling algorithm. We show examples of this in <a 
href="#x1-6r2">Figure&#x00A0;2</a> and
<a 
href="#x1-48r10">Figure&#x00A0;10</a>.
<a 
 id="x1-51r2"></a>
<span 
class="ptmrc7t-">3.3.2.</span><span 
class="ptmrc7t-">&#x00A0;</span><span 
class="ptmrc7t-">Z<span 
class="small-caps">e</span><span 
class="small-caps">r</span><span 
class="small-caps">o</span>-<span 
class="small-caps">s</span><span 
class="small-caps">h</span><span 
class="small-caps">o</span><span 
class="small-caps">t</span></span>
      <span 
class="ptmrc7t-">M<span 
class="small-caps">a</span><span 
class="small-caps">s</span><span 
class="small-caps">k</span>-<span 
class="small-caps">f</span><span 
class="small-caps">r</span><span 
class="small-caps">e</span><span 
class="small-caps">e</span></span>
      <span 
class="ptmrc7t-"><span 
class="small-caps">e</span><span 
class="small-caps">d</span><span 
class="small-caps">i</span><span 
class="small-caps">t</span><span 
class="small-caps">i</span><span 
class="small-caps">n</span><span 
class="small-caps">g</span></span>
<a 
 id="Q1-1-18"></a>
We use Muse&#x00A0;in a zero-shot sense for mask-free image editing of real input images. This method works directly on the (tokenized)
image and does not require &#8220;inverting&#8221; the full generative process, in contrast with recent zero-shot image editing
techniques leveraging generative models (<a 
href="#Xgal2022stylegan">Gal et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xgal2022stylegan">2022b</a>;&#x00A0;<a 
href="#Xpatashnik2021styleclip">Patashnik et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xpatashnik2021styleclip">2021</a>;&#x00A0;<a 
href="#Xkim2022diffusionclip">Kim et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xkim2022diffusionclip">2022</a>;&#x00A0;<a 
href="#Xnulltext2022">Mokady
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xnulltext2022">2022</a>).
We first convert an input image into visual tokens. Next, we iteratively mask and resample a random subset of tokens, conditioned
on text prompts. We can think of this as being analogous to a Gibbs sampling procedure, where we fix some tokens and resample
others conditioned on them. This has the effect of moving the tokenized image into the typical set of the conditional distribution of
images given a text prompt.
We perform the editing using the low-resolution base model, then perform superres on the final output (conditioned on the editing
prompt). In the examples (<a 
href="#x1-6r2">Figure&#x00A0;2</a>, <a 
href="#x1-52r11">Figure&#x00A0;11</a>), we resample 8% of the tokens per iteration for 100 iterations,
with a guidance scale of 4. We also perform top-<span 
class="cmmi-10">k </span>(<span 
class="cmmi-10">k </span><span 
class="cmr-10">= 3</span>) sampling on the token logits to prevent the process from
diverging too much from the input. The iterative nature allows for control over the final output. <a 
href="#x1-54r12">Figure&#x00A0;12</a> shows
a few intermediate edits (without superres); in this example, the user may prefer iteration 50 or 75 over the final
output.
                                                                                             
                                                                                             
<a 
 id="x1-52r11"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
 <table id="TBL-9" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-9-1g"><col 
id="TBL-9-1"><col 
id="TBL-9-2"><col 
id="TBL-9-3"><col 
id="TBL-9-4"><col 
id="TBL-9-5"><col 
id="TBL-9-6"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-9-1-"><td  style="white-space:normal; text-align:left;" id="TBL-9-1-1"  
class="td11"> <!--l. 45--><p class="noindent" ><span class="rotatebox" style="transform: rotate(-90deg);"> &#x00A0;      Input image </span> </td><td  style="white-space:normal; text-align:left;" id="TBL-9-1-2"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/68_seed_1_orig_sr.jpg" alt="PIC"  
width="85" height="85" >             </td><td  style="white-space:normal; text-align:left;" id="TBL-9-1-3"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/18_seed_0_orig_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-1-4"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/06_seed_6_orig_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-1-5"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/19_seed_2_orig_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-1-6"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/21_seed_3_orig_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-9-2-"><td  style="white-space:normal; text-align:left;" id="TBL-9-2-1"  
class="td11"> <!--l. 45--><p class="noindent" ><span class="rotatebox" style="transform: rotate(-90deg);"> &#x00A0;      Editing output </span> </td><td  style="white-space:normal; text-align:left;" id="TBL-9-2-2"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/68_seed_1_synth_sr.jpg" alt="PIC"  
width="85" height="85" >          </td><td  style="white-space:normal; text-align:left;" id="TBL-9-2-3"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/18_seed_0_synth_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-2-4"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/06_seed_6_synth_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-2-5"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/19_seed_2_synth_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-2-6"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/21_seed_3_synth_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-9-3-"><td  style="white-space:normal; text-align:left;" id="TBL-9-3-1"  
class="td11"> <!--l. 45--><p class="noindent" > </td><td  style="white-space:normal; text-align:left;" id="TBL-9-3-2"  
class="td11"> <!--l. 45--><p class="noindent" >A  bottle  of  Pinot
  Grigio   next   to   a
  glass of white wine
  and a cork.               </td><td  style="white-space:normal; text-align:left;" id="TBL-9-3-3"  
class="td11"> <!--l. 45--><p class="noindent" >A croissant next to
  a latte with a flower
  latte art.                    </td><td  style="white-space:normal; text-align:left;" id="TBL-9-3-4"  
class="td11"> <!--l. 45--><p class="noindent" >A dog.                      </td><td  style="white-space:normal; text-align:left;" id="TBL-9-3-5"  
class="td11"> <!--l. 45--><p class="noindent" >A brown rabbit.        </td><td  style="white-space:normal; text-align:left;" id="TBL-9-3-6"  
class="td11"> <!--l. 45--><p class="noindent" >Bond Street.             </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-9-4-"><td  style="white-space:normal; text-align:left;" id="TBL-9-4-1"  
class="td11"> <!--l. 45--><p class="noindent" ><span class="rotatebox" style="transform: rotate(-90deg);"> &#x00A0;      Input image </span> </td><td  style="white-space:normal; text-align:left;" id="TBL-9-4-2"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/01_seed_0_orig_sr.jpg" alt="PIC"  
width="85" height="85" >              </td><td  style="white-space:normal; text-align:left;" id="TBL-9-4-3"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/65_seed_7_orig_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-4-4"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/27_seed_2_orig_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-4-5"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/43_seed_7_orig_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-4-6"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/59_seed_6_orig_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-9-5-"><td  style="white-space:normal; text-align:left;" id="TBL-9-5-1"  
class="td11"> <!--l. 45--><p class="noindent" ><span class="rotatebox" style="transform: rotate(-90deg);"> &#x00A0;      Editing output </span> </td><td  style="white-space:normal; text-align:left;" id="TBL-9-5-2"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/01_seed_0_synth_sr.jpg" alt="PIC"  
width="85" height="85" >          </td><td  style="white-space:normal; text-align:left;" id="TBL-9-5-3"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/65_seed_7_synth_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-5-4"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/27_seed_2_synth_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-5-5"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/43_seed_7_synth_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td><td  style="white-space:normal; text-align:left;" id="TBL-9-5-6"  
class="td11"> <!--l. 45--><p class="noindent" ><img 
src="figs/mfe/59_seed_6_synth_sr.jpg" alt="PIC"  
width="85" height="85" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-9-6-"><td  style="white-space:normal; text-align:left;" id="TBL-9-6-1"  
class="td11"> <!--l. 45--><p class="noindent" > </td><td  style="white-space:normal; text-align:left;" id="TBL-9-6-2"  
class="td11"> <!--l. 45--><p class="noindent" >A Shiba Inu              </td><td  style="white-space:normal; text-align:left;" id="TBL-9-6-3"  
class="td11"> <!--l. 45--><p class="noindent" >A                      dog
  holding  a  football
  in its mouth              </td><td  style="white-space:normal; text-align:left;" id="TBL-9-6-4"  
class="td11"> <!--l. 45--><p class="noindent" >A       basket       of
  oranges                     </td><td  style="white-space:normal; text-align:left;" id="TBL-9-6-5"  
class="td11"> <!--l. 45--><p class="noindent" >A  photo  of  a  cat
  yawning                   </td><td  style="white-space:normal; text-align:left;" id="TBL-9-6-6"  
class="td11"> <!--l. 45--><p class="noindent" >A  photo  of  a  vase
  of red roses              </td>
</tr></table>
<a 
 id="x1-53"></a>
 <div class="caption" 
><span class="id">Figure 11: </span><span  
class="content">Examples of zero-shot mask-free image editing, post superres. We see that the pose and overall structure of the
image is maintained while changing some specific aspects of the object based on the text prompt.</span></div><!--tex4ht:label?: x1-52r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
                                                                                             
                                                                                             
<a 
 id="x1-54r12"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="tabular"> <table id="TBL-10" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-10-1g"><col 
id="TBL-10-1"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-10-1-"><td  style="white-space:normal; text-align:left;" id="TBL-10-1-1"  
class="td11"> <!--l. 133--><p class="noindent" >&#x00A0;       <img 
src="figs/mfe/18_seed_0_timeline.jpg" alt="PIC"  
width="426" height="426" >                                                                                                                                                                             </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-10-2-"><td  style="white-space:normal; text-align:left;" id="TBL-10-2-1"  
class="td11"> <!--l. 135--><p class="noindent" >&#x00A0; <table id="TBL-13" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-13-1g"><col 
id="TBL-13-1"><col 
id="TBL-13-2"><col 
id="TBL-13-3"><col 
id="TBL-13-4"><col 
id="TBL-13-5"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-13-1-"><td  style="white-space:normal; text-align:left;" id="TBL-13-1-1"  
class="td11"><!--l. 142--><p class="noindent" >Input </td> <td  style="white-space:normal; text-align:left;" id="TBL-13-1-2"  
class="td11"><!--l. 142--><p class="noindent" >Iteration 25 </td> <td  style="white-space:normal; text-align:left;" id="TBL-13-1-3"  
class="td11"><!--l. 142--><p class="noindent" >Iteration 50 </td> <td  style="white-space:normal; text-align:left;" id="TBL-13-1-4"  
class="td11"><!--l. 142--><p class="noindent" >Iteration 75 </td> <td  style="white-space:normal; text-align:left;" id="TBL-13-1-5"  
class="td11"><!--l. 142--><p class="noindent" >Iteration 100 </td>
       </tr></table>                                                                                                                                                                                    </td>
</tr></table></div>
<a 
 id="x1-55"></a>
<br /> <div class="caption" 
><span class="id">Figure 12: </span><span  
class="content">Intermediate iterations producing one of the edits in <a 
href="#x1-52r11">Figure&#x00A0;11</a> (pre-superres)</span></div><!--tex4ht:label?: x1-54r -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
<a 
 id="x1-56r4"></a>
<span 
class="ptmb7t-x-x-120">4.</span><span 
class="ptmb7t-x-x-120">&#x00A0;</span><span 
class="ptmb7t-x-x-120">Related</span>
   <span 
class="ptmb7t-x-x-120">Work</span>
<a 
 id="Q1-1-19"></a>
<a 
 id="x1-57r1"></a>
<span 
class="ptmb7t-">4.1.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Image</span>
    <span 
class="ptmb7t-">Generation</span>
    <span 
class="ptmb7t-">Models</span>
<a 
 id="Q1-1-20"></a>
Variational autoencoders (<a 
href="#Xvqvae">Van Den&#x00A0;Oord et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xvqvae">2017</a>) and Generative Adversarial Models (GANs) have shown excellent image
generation performance with many variants proposed for both convolutional and Transformer architectures e.g. (<a 
href="#Xgoodfellow2020generative">Goodfellow
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xgoodfellow2020generative">2020</a>;&#x00A0;<a 
href="#Xesser2021taming">Esser et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xesser2021taming">2021b</a>;&#x00A0;<a 
href="#Xkarras2019style">Karras et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xkarras2019style">2019</a>;&#x00A0;<a 
href="#Xbrock2018large">Brock et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xbrock2018large">2018</a>;&#x00A0;<a 
href="#Xdonahue2019large">Donahue &amp; Simonyan</a>,&#x00A0;<a 
href="#Xdonahue2019large">2019</a>). Until recently,
GANs were considered state of the art. Diffusion models, based on progressive denoising principles, are now able to
synthesize images and video at equal or higher fidelity (<a 
href="#Xddpm">Ho et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xddpm">2020</a>;&#x00A0;<a 
href="#Xkingma2021variational">Kingma et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xkingma2021variational">2021</a>;&#x00A0;<a 
href="#Xho2022video">Ho et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xho2022video">2022</a>).
Hybrid approaches that combine principles from multiple approaches have also shown excellent performance (<a 
href="#Xmaskgit">Chang
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmaskgit">2022</a>;&#x00A0;<a 
href="#Xlezama2022improved">Lezama et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xlezama2022improved">2022</a>), suggesting that there are more complementarities between approaches that can be
exploited.
<a 
 id="x1-58r2"></a>
<span 
class="ptmb7t-">4.2.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Image</span>
    <span 
class="ptmb7t-">Tokenizers</span>
<a 
 id="Q1-1-21"></a>
Image tokenizers are proving to be useful for multiple generative models due to the ability to move the bulk of the
computation from input (pixel) space to latents (<a 
href="#Xldm">Rombach et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>), or to enabling more effective loss functions such
as classification instead of regression (<a 
href="#Xmaskgit">Chang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmaskgit">2022</a>;&#x00A0;<a 
href="#Xlezama2022improved">Lezama et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xlezama2022improved">2022</a>;&#x00A0;<a 
href="#Xli2022mage">Li et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xli2022mage">2022</a>). A number of
tokenization approaches such as Discrete VAE&#8217;s (<a 
href="#Xrolfe2016discrete">Rolfe</a>,&#x00A0;<a 
href="#Xrolfe2016discrete">2016</a>), VQVAE (<a 
href="#Xvqvae">Van Den&#x00A0;Oord et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xvqvae">2017</a>) and VQGAN
(<a 
href="#Xesser2021taming">Esser et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xesser2021taming">2021b</a>) have been developed, with the latter being the highest-performing as it combines perceptual
and adversarial losses to achieve excellent reconstruction. ViT-VQGAN (<a 
href="#Xyu2021vector">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xyu2021vector">2021</a>) extends VQGAN to the
Transformer architecture. We use VQGAN rather than ViT-VQGAN as we found it to perform better for our model,
noting that a better performing tokenization model does not always translate to a better performing text-to-image
model.
<a 
 id="x1-59r3"></a>
<span 
class="ptmb7t-">4.3.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Large</span>
    <span 
class="ptmb7t-">Language</span>
    <span 
class="ptmb7t-">Models</span>
<a 
 id="Q1-1-22"></a>
Our work leverages T5, a pre-trained large language model (LLM) that has been trained on multiple text-to-text tasks (<a 
href="#Xt5xxl">Raffel
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xt5xxl">2020</a>). LLMs (including T5, BERT (<a 
href="#Xbert">Devlin et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xbert">2018</a>), and GPT (<a 
href="#Xbrown2020language">Brown et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xbrown2020language">2020</a>;&#x00A0;<a 
href="#Xradford2019language">Radford et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xradford2019language">2019</a>)) have
been shown to learn powerful embeddings which enable few-shot transfer learning. We leverage this capacity in our
model. All of the modern LLMs are trained on token prediction tasks (either autoregressive or not). The insights
regarding the power of token prediction is leveraged in this work, where we apply a transformer to predict <span 
class="ptmri7t-">visual</span>
tokens.
<a 
 id="x1-60r4"></a>
<span 
class="ptmb7t-">4.4.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Text-Image</span>
    <span 
class="ptmb7t-">Models</span>
                                                                                             
                                                                                             
<a 
 id="Q1-1-23"></a>
Leveraging paired text-image data is proving to be a powerful learning paradigm for representation learning and generative models.
CLIP (<a 
href="#Xclip">Radford et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xclip">2021</a>) and ALIGN (<a 
href="#Xjia2021scaling">Jia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xjia2021scaling">2021</a>) train models to align pairs of text and image embeddings, showing
excellent transfer and few-shot capabilities. Imagen (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>) and Parti (<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>) use similar large scale
text-image datasets (<a 
href="#Xlaion">Schuhmann et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xlaion">2021</a>;&#x00A0;<a 
href="#Xschuhmann2022laion">2022</a>) to learn how to predict images from text inputs, achieving excellent results on
FID and human evaluations. A key trick is the use of classifier-free guidance (<a 
href="#Xho2022classifier">Ho &amp; Salimans</a>,&#x00A0;<a 
href="#Xho2022classifier">2022</a>;&#x00A0;<a 
href="#Xdhariwal2021diffusion">Dhariwal &amp; Nichol</a>,&#x00A0;<a 
href="#Xdhariwal2021diffusion">2021</a>)
that trades off diversity and quality.
<a 
 id="x1-61r5"></a>
<span 
class="ptmb7t-">4.5.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Image</span>
    <span 
class="ptmb7t-">Editing</span>
    <span 
class="ptmb7t-">with</span>
    <span 
class="ptmb7t-">Generative</span>
    <span 
class="ptmb7t-">Models</span>
<a 
 id="Q1-1-24"></a>
GANs have been extensively studied for image editing and manipulation capabilities (see (<a 
href="#Xxia2022gan">Xia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xxia2022gan">2022</a>) for a survey). A number
of techniques have been developed on diffusion models to enable editing, personalization and inversion to token space (<a 
href="#Xgal2022image">Gal
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xgal2022image">2022a</a>;&#x00A0;<a 
href="#Xmeng2021sdedit">Meng et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xmeng2021sdedit">2021</a>;&#x00A0;<a 
href="#Xdreambooth">Ruiz et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdreambooth">2022</a>;&#x00A0;<a 
href="#Ximagic">Kawar et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagic">2022</a>;&#x00A0;<a 
href="#Xbrooks2022instructpix2pix">Brooks et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xbrooks2022instructpix2pix">2022</a>;&#x00A0;<a 
href="#Xprompttoprompt">Hertz et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xprompttoprompt">2022</a>;&#x00A0;<a 
href="#Xnulltext2022">Mokady
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xnulltext2022">2022</a>). Dreambooth (<a 
href="#Xdreambooth">Ruiz et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xdreambooth">2022</a>) and Imagic (<a 
href="#Ximagic">Kawar et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagic">2022</a>) involve fine-tuning of the generative models.
ImagenEditor (<a 
href="#Ximageneditor">Wang et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximageneditor">2022</a>) frames the editing task as text-guided image inpainting, and involves user specified
masks.
<a 
 id="x1-62r5"></a>
<span 
class="ptmb7t-x-x-120">5.</span><span 
class="ptmb7t-x-x-120">&#x00A0;</span><span 
class="ptmb7t-x-x-120">Discussion</span>
   <span 
class="ptmb7t-x-x-120">and</span>
   <span 
class="ptmb7t-x-x-120">Social</span>
   <span 
class="ptmb7t-x-x-120">Impact</span>
<a 
 id="Q1-1-25"></a>
The Muse&#x00A0;model confirms the findings of (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>) that frozen large pretrained language models serve as powerful text
encoders for text-to-image generation. We also tried in our initial experiments to learn a language model from scratch on the training
data, but found that performance was significantly worse than using a pre-trained LLM, especially on long prompts and rare words.
We also show that non-diffusion, non-autoregressive models based on the Transformer architecture can perform at par with diffusion
models while being significantly more efficient at inference time. We achieve SOTA CLIP scores, showing an excellent
alignment beteween image and text. We also show the flexibility of our approach with a number of image editing
applications.
We recognize that generative models have a number of applications with varied potential for impact on human society. Generative
models (<a 
href="#Ximagen">Saharia et&#x00A0;al.</a>,&#x00A0;<a 
href="#Ximagen">2022</a>;&#x00A0;<a 
href="#Xparti">Yu et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xparti">2022</a>;&#x00A0;<a 
href="#Xldm">Rombach et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xldm">2022</a>;&#x00A0;<a 
href="#Xmidjourney">Midjourney</a>,&#x00A0;<a 
href="#Xmidjourney">2022</a>) hold significant potential to augment
human creativity (<a 
href="#Xhughes2021generative">Hughes et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xhughes2021generative">2021</a>). However, it is well known that they can also be leveraged for misinformation, harassment
and various types of social and cultural biases (<a 
href="#Xfranks2018sex">Franks &amp; Waldman</a>,&#x00A0;<a 
href="#Xfranks2018sex">2018</a>;&#x00A0;<a 
href="#Xwhittaker2020all">Whittaker et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xwhittaker2020all">2020</a>;&#x00A0;<a 
href="#Xsrinivasan2021biases">Srinivasan &amp;
Uchino</a>,&#x00A0;<a 
href="#Xsrinivasan2021biases">2021</a>;&#x00A0;<a 
href="#Xsteed2021image">Steed &amp; Caliskan</a>,&#x00A0;<a 
href="#Xsteed2021image">2021</a>). Due to these important considerations, we opt to not release code or a public demo at this
point in time.
Dataset biases are another important ethical consideration due to the requirement of large datasets that are mostly automatically
curated. Such datasets have various potentially problematic issues such as consent and subject awareness (<a 
href="#Xpaullada2021data">Paullada
et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xpaullada2021data">2021</a>;&#x00A0;<a 
href="#Xdulhanty2020issues">Dulhanty</a>,&#x00A0;<a 
href="#Xdulhanty2020issues">2020</a>;&#x00A0;<a 
href="#Xscheuerman2021datasets">Scheuerman et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xscheuerman2021datasets">2021</a>). Many of the commonly used datasets tend to reflect negative social
stereotypes and viewpoints (<a 
href="#Xprabhu2020large">Prabhu &amp; Birhane</a>,&#x00A0;<a 
href="#Xprabhu2020large">2020</a>). Thus, it is quite feasible that training on such datasets simply amplifies these
biases and significant additional research is required on how to mitigate such biases, and generate datasets that are free of them: this
is a very important topic (<a 
href="#Xbuolamwini2018gender">Buolamwini &amp; Gebru</a>,&#x00A0;<a 
href="#Xbuolamwini2018gender">2018</a>;&#x00A0;<a 
href="#Xhendricks2018women">Hendricks et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xhendricks2018women">2018</a>) that is out of the scope of this
paper.
Given the above considerations, we do not recommend the use of text-to-image generation models without attention to the various
use cases and an understanding of the potential for harm. We especially caution against using such models for generation of people,
                                                                                             
                                                                                             
humans and faces.
<h3 class="likesectionHead"><a 
 id="x1-10005"></a>Acknowledgements</h3>
<!--l. 240--><p class="noindent" >We thank William Chan, Chitwan Saharia, and Mohammad Norouzi for providing us training datasets, various evaluation codes and
generous suggestions. Jay Yagnik, Rahul Sukthankar, Tom Duerig and David Salesin provided enthusiastic support of this project
for which we are grateful. We thank Victor Gomes and Erica Moreira for infrastructure support, Jing Yu Koh and
Jason Baldridge for dataset, model and evaluation discussions and feedback on the paper, Mike Krainin for model
speedup discussions, JD Velasquez for discussions and insights, Sarah Laszlo, Kathy Meier-Hellstern, and Rachel
Stigler for assisting us with the publication process, Andrew Bunner, Jordi Pont-Tuset, and Shai Noy for help on
internal demos, David Fleet, Saurabh Saxena, Jiahui Yu, and Jason Baldridge for sharing Imagen and Parti speed
metrics.
<!--l. 1--><p class="noindent" >
<h3 class="likesectionHead"><a 
 id="x1-20005"></a>References</h3>
<!--l. 1--><p class="noindent" >
  <div class="thebibliography">
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xaustin2021structured"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Austin, J., Johnson, D.&#x00A0;D., Ho, J., Tarlow, D., and van&#x00A0;den Berg, R. Structured denoising diffusion models in discrete
  state-spaces. <span 
class="ptmri7t-">Advances in Neural Information Processing Systems</span>, 34:17981&#8211;17993, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbrock2018large"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Brock, A., Donahue, J., and Simonyan, K.  Large scale gan training for high fidelity natural image synthesis.  <span 
class="ptmri7t-">arXiv</span>
  <span 
class="ptmri7t-">preprint arXiv:1809.11096</span>, 2018.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbrooks2022instructpix2pix"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Brooks, T., Holynski, A., and Efros, A.&#x00A0;A.   Instructpix2pix: Learning to follow image editing instructions.   <span 
class="ptmri7t-">arXiv</span>
  <span 
class="ptmri7t-">preprint arXiv:2211.09800</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbrown2020language"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.&#x00A0;D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
  Askell, A., et&#x00A0;al.   Language models are few-shot learners.   <span 
class="ptmri7t-">Advances in neural information processing systems</span>, 33:
  1877&#8211;1901, 2020.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbuolamwini2018gender"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Buolamwini, J. and Gebru, T.  Gender shades: Intersectional accuracy disparities in commercial gender classification.
  In <span 
class="ptmri7t-">Conference on fairness, accountability and transparency</span>, pp.&#x00A0;77&#8211;91. PMLR, 2018.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmaskgit"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W.&#x00A0;T.   Maskgit: Masked generative image transformer.   In
  <span 
class="ptmri7t-">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp.&#x00A0;11315&#8211;11325, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsdgeneration"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>CompVis. Stable diffusion colab, 2022. URL <a 
href="https://colab.sandbox.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=zHkHsdtnry57" class="url" ><span 
class="pcrr7t-">https://colab.sandbox.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb#scrollTo=zHkHsdtnry57</span></a>.
                                                                                             
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbert"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language
  understanding. <span 
class="ptmri7t-">arXiv preprint arXiv:1810.04805</span>, 2018.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdhariwal2021diffusion"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. <span 
class="ptmri7t-">Advances in Neural Information Processing</span>
  <span 
class="ptmri7t-">Systems</span>, 34:8780&#8211;8794, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdonahue2019large"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Donahue,  J.  and  Simonyan,  K.    Large  scale  adversarial  representation  learning.    <span 
class="ptmri7t-">Advances  in  neural  information</span>
  <span 
class="ptmri7t-">processing systems</span>, 32, 2019.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdulhanty2020issues"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Dulhanty, C.  Issues in computer vision data collection: Bias, consent, and label taxonomy.  Master&#8217;s thesis, University
  of Waterloo, 2020.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xesser2021imagebart"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Esser, P., Rombach, R., Blattmann, A., and Ommer, B. Imagebart: Bidirectional context with multinomial diffusion for
  autoregressive image synthesis. <span 
class="ptmri7t-">Advances in Neural Information Processing Systems</span>, 34:3518&#8211;3532, 2021a.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xesser2021taming"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Esser, P., Rombach, R., and Ommer, B.  Taming transformers for high-resolution image synthesis.  In <span 
class="ptmri7t-">Proceedings of</span>
  <span 
class="ptmri7t-">the IEEE/CVF conference on computer vision and pattern recognition</span>, pp.&#x00A0;12873&#8211;12883, 2021b.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xfranks2018sex"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Franks, M.&#x00A0;A. and Waldman, A.&#x00A0;E.  Sex, lies, and videotape: Deep fakes and free speech delusions.  <span 
class="ptmri7t-">Md. L. Rev.</span>, 78:
  892, 2018.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmakeascene"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., and Taigman, Y.  Make-a-scene: Scene-based text-to-image
  generation with human priors, 2022. URL <a 
href="https://arxiv.org/abs/2203.13131" class="url" ><span 
class="pcrr7t-">https://arxiv.org/abs/2203.13131</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xgal2022image"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.&#x00A0;H., Chechik, G., and Cohen-Or, D. An image is worth one
  word: Personalizing text-to-image generation using textual inversion. <span 
class="ptmri7t-">arXiv preprint arXiv:2208.01618</span>, 2022a.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xgal2022stylegan"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Gal, R., Patashnik, O., Maron, H., Bermano, A.&#x00A0;H., Chechik, G., and Cohen-Or, D. Stylegan-nada: Clip-guided domain
  adaptation of image generators. <span 
class="ptmri7t-">ACM Transactions on Graphics (TOG)</span>, 41(4):1&#8211;13, 2022b.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xgoodfellow2020generative"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Goodfellow,  I.,  Pouget-Abadie,  J.,  Mirza,  M.,  Xu,  B.,  Warde-Farley,  D.,  Ozair,  S.,  Courville,  A.,  and  Bengio,  Y.
  Generative adversarial networks. <span 
class="ptmri7t-">Communications of the ACM</span>, 63(11):139&#8211;144, 2020.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="XGoyal2017AccurateLM"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Goyal,  P.,  Dollár,  P.,  Girshick,  R.&#x00A0;B.,  Noordhuis,  P.,  Wesolowski,  L.,  Kyrola,  A.,  Tulloch,  A.,  Jia,  Y.,  and  He,  K.
  Accurate, large minibatch SGD: Training ImageNet in 1 hour. <span 
class="ptmri7t-">preprint arXiv:1706.0267</span>, 2017.                  
                                                                                             
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="XMAE"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., and Girshick, R. Masked autoencoders are scalable vision learners. In <span 
class="ptmri7t-">cvpr</span>,
  pp.&#x00A0;16000&#8211;16009, June 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xhendricks2018women"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Hendricks, L.&#x00A0;A., Burns, K., Saenko, K., Darrell, T., and Rohrbach, A.  Women also snowboard: Overcoming bias in
  captioning models. In <span 
class="ptmri7t-">Proceedings of the European Conference on Computer Vision (ECCV)</span>, pp.&#x00A0;771&#8211;787, 2018.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xprompttoprompt"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., and Cohen-Or, D.  Prompt-to-prompt image editing
  with cross attention control. <span 
class="ptmri7t-">arXiv preprint arXiv:2208.01626</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xfid"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule
  converge to a local nash equilibrium. <span 
class="ptmri7t-">Advances in neural information processing systems</span>, 30, 2017.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xho2022classifier"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ho, J. and Salimans, T. Classifier-free diffusion guidance. <span 
class="ptmri7t-">arXiv preprint arXiv:2207.12598</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xddpm"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ho, J., Jain, A., and Abbeel, P.  Denoising diffusion probabilistic models.  <span 
class="ptmri7t-">Advances in Neural Information Processing</span>
  <span 
class="ptmri7t-">Systems</span>, 33:6840&#8211;6851, 2020.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xho2022video"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D.&#x00A0;J.  Video diffusion models.  <span 
class="ptmri7t-">arXiv preprint</span>
  <span 
class="ptmri7t-">arXiv:2204.03458</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xhughes2021generative"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Hughes,  R.&#x00A0;T.,  Zhu,  L.,  and  Bednarz,  T.    Generative  adversarial  networks&#8211;enabled  human&#8211;artificial  intelligence
  collaborative  applications  for  creative  and  design  industries:  A  systematic  review  of  current  approaches  and  trends.
  <span 
class="ptmri7t-">Frontiers in artificial intelligence</span>, 4:604234, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xjia2021scaling"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T.  Scaling up
  visual and vision-language representation learning with noisy text supervision. In <span 
class="ptmri7t-">International Conference on Machine</span>
  <span 
class="ptmri7t-">Learning</span>, pp.&#x00A0;4904&#8211;4916. PMLR, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xjouppi2020domain"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jouppi, N.&#x00A0;P., Yoon, D.&#x00A0;H., Kurian, G., Li, S., Patil, N., Laudon, J., Young, C., and Patterson, D.  A domain-specific
  supercomputer for training deep neural networks. <span 
class="ptmri7t-">Communications of the ACM</span>, 63(7):67&#8211;78, 2020.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkarras2019style"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Karras,  T.,  Laine,  S.,  and  Aila,  T.    A  style-based  generator  architecture  for  generative  adversarial  networks.    In
  <span 
class="ptmri7t-">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp.&#x00A0;4401&#8211;4410, 2019.
  </p>                                                                                           
                                                                                             
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Ximagic"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., and Irani, M.  Imagic: Text-based real image
  editing with diffusion models. <span 
class="ptmri7t-">arXiv preprint arXiv:2210.09276</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkim2022diffusionclip"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kim, G., Kwon, T., and Ye, J.&#x00A0;C.   Diffusionclip: Text-guided diffusion models for robust image manipulation.   In
  <span 
class="ptmri7t-">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp.&#x00A0;2426&#8211;2435, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkingma2021variational"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kingma,  D.,  Salimans,  T.,  Poole,  B.,  and  Ho,  J.    Variational  diffusion  models.    <span 
class="ptmri7t-">Advances  in  neural  information</span>
  <span 
class="ptmri7t-">processing systems</span>, 34:21696&#8211;21707, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="XKingmaB14"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kingma, D.&#x00A0;P. and Ba, J. Adam: A method for stochastic optimization. In <span 
class="ptmri7t-">ICLR</span>, 2015.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsdinference"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lambda   Labs.        All   you   need   is   one   gpu:   Inference   benchmark   for   stable   diffusion,   2022.        URL
  <a 
href="https://lambdalabs.com/blog/inference-benchmark-stable-diffusion" class="url" ><span 
class="pcrr7t-">https://lambdalabs.com/blog/inference-benchmark-stable-diffusion</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlee2022autoregressive"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lee, D., Kim, C., Kim, S., Cho, M., and Han, W.-S.  Autoregressive image generation using residual quantization.  In
  <span 
class="ptmri7t-">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp.&#x00A0;11523&#8211;11532, 2022a.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlee2022draft"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lee, D., Kim, C., Kim, S., Cho, M., and Han, W.-S.   Draft-and-revise: Effective image generation with contextual
  rq-transformer. <span 
class="ptmri7t-">arXiv preprint arXiv:2206.04452</span>, 2022b.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlezama2022improved"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lezama, J., Chang, H., Jiang, L., and Essa, I.   Improved masked image generation with token-critic.   In <span 
class="ptmri7t-">European</span>
  <span 
class="ptmri7t-">Conference on Computer Vision</span>, pp.&#x00A0;70&#8211;86. Springer, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xli2022mage"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Li, T., Chang, H., Mishra, S.&#x00A0;K., Zhang, H., Katabi, D., and Krishnan, D.  Mage: Masked generative encoder to unify
  representation learning and image synthesis. <span 
class="ptmri7t-">arXiv preprint arXiv:2211.09117</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xcoco"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and Zitnick, C.&#x00A0;L.  Microsoft coco:
  Common objects in context. In <span 
class="ptmri7t-">European conference on computer vision</span>, pp.&#x00A0;740&#8211;755. Springer, 2014.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="XLoshchilov2017SGDRSG"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In <span 
class="ptmri7t-">iclr</span>, 2017.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="XZhu2022dpm"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ODE solver for diffusion probabilistic model
  sampling in around 10 steps. <span 
class="ptmri7t-">arXiv preprint arXiv:2206.00927</span>, 2022.                                      
                                                                                             
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmeng2021sdedit"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S.  Sdedit: Image synthesis and editing with stochastic
  differential equations. <span 
class="ptmri7t-">arXiv preprint arXiv:2108.01073</span>, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmerullo2022linearly"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Merullo, J., Castricato, L., Eickhoff, C., and Pavlick, E.  Linearly mapping from image to text space.  <span 
class="ptmri7t-">arXiv preprint</span>
  <span 
class="ptmri7t-">arXiv:2209.15162</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmidjourney"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Midjourney. Midjourney, 2022. URL <a 
href="https://www.midjourney.com" class="url" ><span 
class="pcrr7t-">https://www.midjourney.com</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xnulltext2022"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mokady, R., Hertz, A., Aberman, K., Pritch, Y., and Cohen-Or, D.  Null-text inversion for editing real images using
  guided diffusion models, 2022. URL <a 
href="https://arxiv.org/abs/2211.09794" class="url" ><span 
class="pcrr7t-">https://arxiv.org/abs/2211.09794</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xnegprompt"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>NegPrompt.                                                Negative               prompt,               2022.                                                URL
  <a 
href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Negative-prompt" class="url" ><span 
class="pcrr7t-">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Negative-prompt</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xglide"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards
  photorealistic image generation and editing with text-guided diffusion models. <span 
class="ptmri7t-">arXiv preprint arXiv:2112.10741</span>, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpatashnik2021styleclip"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Patashnik, O., Wu, Z., Shechtman, E., Cohen-Or, D., and Lischinski, D. Styleclip: Text-driven manipulation of stylegan
  imagery. In <span 
class="ptmri7t-">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pp.&#x00A0;2085&#8211;2094, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpaullada2021data"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Paullada, A., Raji, I.&#x00A0;D., Bender, E.&#x00A0;M., Denton, E., and Hanna, A.  Data and its (dis) contents: A survey of dataset
  development and use in machine learning research. <span 
class="ptmri7t-">Patterns</span>, 2(11):100336, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xprabhu2020large"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Prabhu,  V.&#x00A0;U.  and  Birhane,  A.     Large  image  datasets:  A  pyrrhic  win  for  computer  vision?     <span 
class="ptmri7t-">arXiv  preprint</span>
  <span 
class="ptmri7t-">arXiv:2006.16923</span>, 2020.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xradford2019language"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et&#x00A0;al. Language models are unsupervised multitask
  learners. <span 
class="ptmri7t-">OpenAI blog</span>, 1(8):9, 2019.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xclip"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Radford, A., Kim, J.&#x00A0;W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
  et&#x00A0;al.  Learning transferable visual models from natural language supervision.  In <span 
class="ptmri7t-">International Conference on Machine</span>
  <span 
class="ptmri7t-">Learning</span>, pp.&#x00A0;8748&#8211;8763. PMLR, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xt5xxl"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.&#x00A0;J., et&#x00A0;al.  Exploring the
  limits of transfer learning with a unified text-to-text transformer. <span 
class="ptmri7t-">J. Mach. Learn. Res.</span>, 21(140):1&#8211;67, 2020.         
                                                                                             
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdalle1"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image
  generation, 2021. URL <a 
href="https://arxiv.org/abs/2102.12092" class="url" ><span 
class="pcrr7t-">https://arxiv.org/abs/2102.12092</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdalle2"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.  Hierarchical text-conditional image generation with clip
  latents. <span 
class="ptmri7t-">arXiv preprint arXiv:2204.06125</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrolfe2016discrete"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Rolfe, J.&#x00A0;T. Discrete variational autoencoders. <span 
class="ptmri7t-">arXiv preprint arXiv:1609.02200</span>, 2016.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xldm"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion
  models.  In <span 
class="ptmri7t-">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pp.&#x00A0;10684&#8211;10695,
  2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdreambooth"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K.   Dreambooth: Fine tuning text-to-image
  diffusion models for subject-driven generation. <span 
class="ptmri7t-">arXiv preprint arXiv:2208.12242</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Ximagen"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K.&#x00A0;S., Ayan, B.&#x00A0;K., Mahdavi, S.&#x00A0;S.,
  Lopes, R.&#x00A0;G., et&#x00A0;al.  Photorealistic text-to-image diffusion models with deep language understanding.  <span 
class="ptmri7t-">arXiv preprint</span>
  <span 
class="ptmri7t-">arXiv:2205.11487</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsalimans2022distillation"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In <span 
class="ptmri7t-">ICLR</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xscheuerman2021datasets"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Scheuerman, M.&#x00A0;K., Hanna, A., and Denton, E.   Do datasets have politics? disciplinary values in computer vision
  dataset development. <span 
class="ptmri7t-">Proceedings of the ACM on Human-Computer Interaction</span>, 5(CSCW2):1&#8211;37, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlaion"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Schuhmann,  C.,  Vencu,  R.,  Beaumont,  R.,  Kaczmarczyk,  R.,  Mullis,  C.,  Katta,  A.,  Coombes,  T.,  Jitsev,  J.,
  and  Komatsuzaki,  A.     Laion-400m:  Open  dataset  of  clip-filtered  400  million  image-text  pairs.     <span 
class="ptmri7t-">arXiv  preprint</span>
  <span 
class="ptmri7t-">arXiv:2111.02114</span>, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xschuhmann2022laion"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C.,
  Wortsman, M., et&#x00A0;al.   Laion-5b: An open large-scale dataset for training next generation image-text models.   <span 
class="ptmri7t-">arXiv</span>
  <span 
class="ptmri7t-">preprint arXiv:2210.08402</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsharma2018conceptual"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Sharma,  P.,  Ding,  N.,  Goodman,  S.,  and  Soricut,  R.   Conceptual  captions:  A  cleaned,  hypernymed,  image  alt-text
  dataset for automatic image captioning. In <span 
class="ptmri7t-">Proceedings of the 56th Annual Meeting of the Association for Computational</span>
  <span 
class="ptmri7t-">Linguistics (Volume 1: Long Papers)</span>, pp.&#x00A0;2556&#8211;2565, 2018.                                             
                                                                                             
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xshazeer2018adafactor"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. In <span 
class="ptmri7t-">International Conference</span>
  <span 
class="ptmri7t-">on Machine Learning</span>, pp.&#x00A0;4596&#8211;4604. PMLR, 2018.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsrinivasan2021biases"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Srinivasan, R. and Uchino, K. Biases in generative art: A causal look from the lens of art history. In <span 
class="ptmri7t-">Proceedings of the</span>
  <span 
class="ptmri7t-">2021 ACM Conference on Fairness, Accountability, and Transparency</span>, pp.&#x00A0;41&#8211;51, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsteed2021image"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Steed, R. and Caliskan, A. Image representations learned with unsupervised pre-training contain human-like biases. In
  <span 
class="ptmri7t-">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</span>, pp.&#x00A0;701&#8211;713, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdfgan"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tao, M., Tang, H., Wu, F., Jing, X.-Y., Bao, B.-K., and Xu, C. Df-gan: A simple and effective baseline for text-to-image
  synthesis, 2020. URL <a 
href="https://arxiv.org/abs/2008.05865" class="url" ><span 
class="pcrr7t-">https://arxiv.org/abs/2008.05865</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xvqvae"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Van  Den&#x00A0;Oord,  A.,  Vinyals,  O.,  et&#x00A0;al.    Neural  discrete  representation  learning.    <span 
class="ptmri7t-">Advances  in  neural  information</span>
  <span 
class="ptmri7t-">processing systems</span>, 30, 2017.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xvaswani2017attention"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.&#x00A0;N., Kaiser, &#321;., and Polosukhin, I.  Attention
  is all you need. <span 
class="ptmri7t-">Advances in neural information processing systems</span>, 30, 2017.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Ximageneditor"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Wang, S., Saharia, C., Montgomery, C., Pont-Tuset, J., Noy, S., Pellegrini, S., Onoe, Y., Laszlo, S., Fleet, D.&#x00A0;J., Soricut,
  R., Baldridge, J., Norouzi, M., Anderson, P., and Chan, W.   Imagen editor and editbench: Advancing and evaluating
  text-guided image inpainting, 2022. URL <a 
href="https://arxiv.org/abs/2212.06909" class="url" ><span 
class="pcrr7t-">https://arxiv.org/abs/2212.06909</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwhittaker2020all"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Whittaker, L., Kietzmann, T.&#x00A0;C., Kietzmann, J., and Dabirian, A.  &#8220;all around me are synthetic faces&#8221;: the mad world
  of ai-generated media. <span 
class="ptmri7t-">IT Professional</span>, 22(5):90&#8211;99, 2020.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xxia2022gan"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Xia, W., Zhang, Y., Yang, Y., Xue, J.-H., Zhou, B., and Yang, M.-H.  Gan inversion: A survey.  <span 
class="ptmri7t-">IEEE Transactions on</span>
  <span 
class="ptmri7t-">Pattern Analysis and Machine Intelligence</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xattngan"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Xu,   T.,   Zhang,   P.,   Huang,   Q.,   Zhang,   H.,   Gan,   Z.,   Huang,   X.,   and   He,   X.      Attngan:   Fine-grained   text
  to  image  generation  with  attentional  generative  adversarial  networks.      <span 
class="ptmri7t-">CoRR</span>,  abs/1711.10485,  2017.      URL
  <a 
href="http://arxiv.org/abs/1711.10485" class="url" ><span 
class="pcrr7t-">http://arxiv.org/abs/1711.10485</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xdmgan-cl"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ye, H., Yang, X., Takac, M., Sunderraman, R., and Ji, S. Improving text-to-image synthesis using contrastive learning,
  2021. URL <a 
href="https://arxiv.org/abs/2107.02423" class="url" ><span 
class="pcrr7t-">https://arxiv.org/abs/2107.02423</span></a>.                                             
                                                                                             
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xyu2021vector"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Yu, J., Li, X., Koh, J.&#x00A0;Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized image
  modeling with improved vqgan. <span 
class="ptmri7t-">arXiv preprint arXiv:2110.04627</span>, 2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xparti"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Yu, J., Xu, Y., Koh, J.&#x00A0;Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B.&#x00A0;K., et&#x00A0;al.  Scaling
  autoregressive models for content-rich text-to-image generation. <span 
class="ptmri7t-">arXiv preprint arXiv:2206.10789</span>, 2022.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xzhang2021cross"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zhang,  H.,  Koh,  J.&#x00A0;Y.,  Baldridge,  J.,  Lee,  H.,  and  Yang,  Y.    Cross-modal  contrastive  learning  for  text-to-image
  generation.   In <span 
class="ptmri7t-">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp.&#x00A0;833&#8211;842,
  2021.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xhit"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zhao, L., Zhang, Z., Chen, T., Metaxas, D.&#x00A0;N., and Zhang, H.  Improved transformer for high-resolution gans, 2021.
  URL <a 
href="https://arxiv.org/abs/2106.07631" class="url" ><span 
class="pcrr7t-">https://arxiv.org/abs/2106.07631</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlafite"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zhou,      Y.,      Zhang,      R.,      Chen,      C.,      Li,      C.,      Tensmeyer,      C.,      Yu,      T.,      Gu,      J.,      Xu,      J.,
  and Sun, T.  LAFITE: towards language-free training for text-to-image generation.  <span 
class="ptmri7t-">CoRR</span>, abs/2111.13792, 2021.  URL
  <a 
href="https://arxiv.org/abs/2111.13792" class="url" ><span 
class="pcrr7t-">https://arxiv.org/abs/2111.13792</span></a>.
  </p>
  <p class="bibitem" ><span class="biblabel">
<a 
 id="Xzhu2019dm"></a><span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Zhu, M., Pan, P., Chen, W., and Yang, Y. Dm-gan: Dynamic memory generative adversarial networks for text-to-image
  synthesis.  In <span 
class="ptmri7t-">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pp.&#x00A0;5802&#8211;5810,
  2019.
</p>
  </div>
<!--l. 246--><p class="noindent" >
                                                                                             
                                                                                             
                                                                                             
                                                                                             
<a 
 id="x1-2001r1"></a>
<span 
class="ptmb7t-x-x-120">A.</span><span 
class="ptmb7t-x-x-120">&#x00A0;</span><span 
class="ptmb7t-x-x-120">Appendix.</span>
<a 
 id="Q1-1-28"></a>
<a 
 id="x1-2002r1"></a>
<span 
class="ptmb7t-">A.1.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Base</span>
    <span 
class="ptmb7t-">Model</span>
    <span 
class="ptmb7t-">Configurations</span>
<a 
 id="Q1-1-29"></a>
Our base model configuration for our largest model of size 3B parameters is given in Table <a 
href="#x1-2003r4">4<!--tex4ht:ref: tab:basemodel --></a>.
<div class="table">
                                                                                             
                                                                                             
<a 
 id="x1-2003r4"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="tabular"> <table id="TBL-14" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-14-1g"><col 
id="TBL-14-1"></colgroup><colgroup id="TBL-14-2g"><col 
id="TBL-14-2"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-14-1-"><td  style="white-space:normal; text-align:left;" id="TBL-14-1-1"  
class="td11"> <!--l. 6--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Configuration</span>                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-14-1-2"  
class="td11"> <!--l. 6--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Value</span>                                                                                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-14-2-"><td  style="white-space:normal; text-align:left;" id="TBL-14-2-1"  
class="td11"> </td></tr><tr><td colspan="2"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-014-2-"><td  style="white-space:normal; text-align:left;" id="TBL-014-2-1"  
class="td11">
 <!--l. 7--><p class="noindent" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-014-3-"><td  style="white-space:normal; text-align:left;" id="TBL-014-3-1"  
class="td11"> <!--l. 7--><p class="noindent" >                           </td></tr><tr 
class="hline"><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-014-4-"><td  style="white-space:normal; text-align:left;" id="TBL-014-4-1"  
class="td11"> </td></tr><tr><td colspan="2"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-4-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-4-1"  
class="td11">
 <!--l. 7--><p class="noindent" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-5-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-5-1"  
class="td11"> <!--l. 7--><p class="noindent" >                           </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-6-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-6-1"  
class="td11"> <!--l. 9--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Number</span>
   <span 
class="ptmr7t-x-x-70">of</span>
   <span 
class="ptmr7t-x-x-70">Transformer</span>
   <span 
class="ptmr7t-x-x-70">layers</span>                                                        </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-6-2"  
class="td11"> <!--l. 9--><p class="noindent" ><span 
class="ptmr7t-x-x-70">48</span>                                                                                     </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-7-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-7-1"  
class="td11"> <!--l. 10--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Transformer</span>
   <span 
class="ptmr7t-x-x-70">Hidden</span>
   <span 
class="ptmr7t-x-x-70">Dimension</span>                                                </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-7-2"  
class="td11"> <!--l. 10--><p class="noindent" ><span 
class="ptmr7t-x-x-70">2048</span>                                                                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-8-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-8-1"  
class="td11"> <!--l. 11--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Transformer</span>
   <span 
class="ptmr7t-x-x-70">MLP</span>
   <span 
class="ptmr7t-x-x-70">Dimension</span>                                                </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-8-2"  
class="td11"> <!--l. 11--><p class="noindent" ><span 
class="ptmr7t-x-x-70">8192</span>                                                                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-9-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-9-1"  
class="td11"> <!--l. 12--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Optimizer</span>                                                 </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-9-2"  
class="td11"> <!--l. 12--><p class="noindent" ><span 
class="ptmr7t-x-x-70">AdaFactor</span>
   <span 
class="ptmr7t-x-x-70">(</span><a 
href="#Xshazeer2018adafactor"><span 
class="ptmr7t-x-x-70">Shazeer</span>
   <span 
class="ptmr7t-x-x-70">&amp;</span>
   <span 
class="ptmr7t-x-x-70">Stern</span></a><span 
class="ptmr7t-x-x-70">,</span><span 
class="ptmr7t-x-x-70">&#x00A0;</span><a 
href="#Xshazeer2018adafactor"><span 
class="ptmr7t-x-x-70">2018</span></a><span 
class="ptmr7t-x-x-70">)</span>                                                                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-10-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-10-1"  
class="td11"> <!--l. 13--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Base</span>
   <span 
class="ptmr7t-x-x-70">learning</span>
   <span 
class="ptmr7t-x-x-70">rate</span>                                                            </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-10-2"  
class="td11"> <!--l. 13--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1e-4</span>                                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-11-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-11-1"  
class="td11"> <!--l. 14--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Weight</span>
   <span 
class="ptmr7t-x-x-70">decay</span>                                                        </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-11-2"  
class="td11"> <!--l. 14--><p class="noindent" ><span 
class="ptmr7t-x-x-70">0.045</span>                                                                                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-12-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-12-1"  
class="td11"> <!--l. 15--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Optimizer</span>
   <span 
class="ptmr7t-x-x-70">momentum</span>                                               </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-12-2"  
class="td11"> <!--l. 15--><p class="noindent" ><span 
class="cmmi-7">&#x03B2;</span><sub><span 
class="cmr-5">1</span></sub><span 
class="cmr-7">=</span><span 
class="cmr-7">0</span><span 
class="cmmi-7">.</span><span 
class="cmr-7">9</span><span 
class="cmmi-7">,&#x03B2;</span><sub><span 
class="cmr-5">2</span></sub><span 
class="cmr-7">=</span><span 
class="cmr-7">0</span><span 
class="cmmi-7">.</span><span 
class="cmr-7">96</span>                                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-13-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-13-1"  
class="td11"> <!--l. 16--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Batch</span>
   <span 
class="ptmr7t-x-x-70">size</span>                                                           </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-13-2"  
class="td11"> <!--l. 16--><p class="noindent" ><span 
class="ptmr7t-x-x-70">512</span>                                                                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-14-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-14-1"  
class="td11"> <!--l. 17--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Learning</span>
   <span 
class="ptmr7t-x-x-70">rate</span>
   <span 
class="ptmr7t-x-x-70">schedule</span>                                                    </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-14-2"  
class="td11"> <!--l. 17--><p class="noindent" ><span 
class="ptmr7t-x-x-70">cosine</span>
   <span 
class="ptmr7t-x-x-70">decay</span>
   <span 
class="ptmr7t-x-x-70">(</span><a 
href="#XLoshchilov2017SGDRSG"><span 
class="ptmr7t-x-x-70">Loshchilov</span>
   <span 
class="ptmr7t-x-x-70">&amp;</span>
   <span 
class="ptmr7t-x-x-70">Hutter</span></a><span 
class="ptmr7t-x-x-70">,</span><span 
class="ptmr7t-x-x-70">&#x00A0;</span><a 
href="#XLoshchilov2017SGDRSG"><span 
class="ptmr7t-x-x-70">2017</span></a><span 
class="ptmr7t-x-x-70">)</span>                                                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-15-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-15-1"  
class="td11"> <!--l. 18--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Warmup</span>
   <span 
class="ptmr7t-x-x-70">steps</span>                                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-15-2"  
class="td11"> <!--l. 18--><p class="noindent" ><span 
class="ptmr7t-x-x-70">5000</span>                                                                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0014-16-"><td  style="white-space:normal; text-align:left;" id="TBL-0014-16-1"  
class="td11"> <!--l. 19--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Training</span>
   <span 
class="ptmr7t-x-x-70">steps</span>                                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-0014-16-2"  
class="td11"> <!--l. 19--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1.5M</span>                                                                                 </td>
</tr></table></div>
<a 
 id="x1-2004"></a>
<br /> <div class="caption" 
><span class="id">Table 4: </span><span  
class="content">Configuration and training hyperparameters for base model.</span></div><!--tex4ht:label?: x1-2003r5 -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
</div>
<a 
 id="x1-2005r2"></a>
<span 
class="ptmb7t-">A.2.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">VQGAN</span>
    <span 
class="ptmb7t-">Configurations</span>
<a 
 id="Q1-1-30"></a>
<div class="table">
                                                                                             
                                                                                             
<a 
 id="x1-2006r5"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="tabular"> <table id="TBL-15" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-15-1g"><col 
id="TBL-15-1"></colgroup><colgroup id="TBL-15-2g"><col 
id="TBL-15-2"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-15-1-"><td  style="white-space:normal; text-align:left;" id="TBL-15-1-1"  
class="td11"> <!--l. 6--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Configuration</span>                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-15-1-2"  
class="td11"> <!--l. 6--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Value</span>                                                                                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-15-2-"><td  style="white-space:normal; text-align:left;" id="TBL-15-2-1"  
class="td11"> </td></tr><tr><td colspan="2"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-015-2-"><td  style="white-space:normal; text-align:left;" id="TBL-015-2-1"  
class="td11">
 <!--l. 7--><p class="noindent" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-015-3-"><td  style="white-space:normal; text-align:left;" id="TBL-015-3-1"  
class="td11"> <!--l. 7--><p class="noindent" >                           </td></tr><tr 
class="hline"><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-015-4-"><td  style="white-space:normal; text-align:left;" id="TBL-015-4-1"  
class="td11"> </td></tr><tr><td colspan="2"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-4-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-4-1"  
class="td11">
 <!--l. 7--><p class="noindent" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-5-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-5-1"  
class="td11"> <!--l. 7--><p class="noindent" >                           </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-6-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-6-1"  
class="td11"> <!--l. 9--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Perceptual</span>
   <span 
class="ptmr7t-x-x-70">loss</span>
   <span 
class="ptmr7t-x-x-70">weight</span>                                                       </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-6-2"  
class="td11"> <!--l. 9--><p class="noindent" ><span 
class="ptmr7t-x-x-70">0.05</span>                                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-7-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-7-1"  
class="td11"> <!--l. 10--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Adversarial</span>
   <span 
class="ptmr7t-x-x-70">loss</span>
   <span 
class="ptmr7t-x-x-70">weight</span>                                                       </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-7-2"  
class="td11"> <!--l. 10--><p class="noindent" ><span 
class="ptmr7t-x-x-70">0.1</span>                                                                                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-8-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-8-1"  
class="td11"> <!--l. 11--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Codebook</span>
   <span 
class="ptmr7t-x-x-70">size</span>                                                           </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-8-2"  
class="td11"> <!--l. 11--><p class="noindent" ><span 
class="ptmr7t-x-x-70">8192</span>                                                                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-9-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-9-1"  
class="td11"> <!--l. 12--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Optimizer</span>                                                 </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-9-2"  
class="td11"> <!--l. 12--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Adam</span>
   <span 
class="ptmr7t-x-x-70">(</span><a 
href="#XKingmaB14"><span 
class="ptmr7t-x-x-70">Kingma</span>
   <span 
class="ptmr7t-x-x-70">&amp;</span>
   <span 
class="ptmr7t-x-x-70">Ba</span></a><span 
class="ptmr7t-x-x-70">,</span><span 
class="ptmr7t-x-x-70">&#x00A0;</span><a 
href="#XKingmaB14"><span 
class="ptmr7t-x-x-70">2015</span></a><span 
class="ptmr7t-x-x-70">)</span>                                                                        </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-10-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-10-1"  
class="td11"> <!--l. 13--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Discriminator</span>
   <span 
class="ptmr7t-x-x-70">learning</span>
   <span 
class="ptmr7t-x-x-70">rate</span>                                                            </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-10-2"  
class="td11"> <!--l. 13--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1e-4</span>                                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-11-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-11-1"  
class="td11"> <!--l. 14--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Generator</span>
   <span 
class="ptmr7t-x-x-70">learning</span>
   <span 
class="ptmr7t-x-x-70">rate</span>                                                            </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-11-2"  
class="td11"> <!--l. 14--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1e-4</span>                                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-12-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-12-1"  
class="td11"> <!--l. 15--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Weight</span>
   <span 
class="ptmr7t-x-x-70">decay</span>                                                        </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-12-2"  
class="td11"> <!--l. 15--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1e-4</span>                                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-13-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-13-1"  
class="td11"> <!--l. 16--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Optimizer</span>
   <span 
class="ptmr7t-x-x-70">momentum</span>                                               </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-13-2"  
class="td11"> <!--l. 16--><p class="noindent" ><span 
class="cmmi-7">&#x03B2;</span><sub><span 
class="cmr-5">1</span></sub><span 
class="cmr-7">=</span><span 
class="cmr-7">0</span><span 
class="cmmi-7">.</span><span 
class="cmr-7">9</span><span 
class="cmmi-7">,&#x03B2;</span><sub><span 
class="cmr-5">2</span></sub><span 
class="cmr-7">=</span><span 
class="cmr-7">0</span><span 
class="cmmi-7">.</span><span 
class="cmr-7">99</span>                                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-14-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-14-1"  
class="td11"> <!--l. 17--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Batch</span>
   <span 
class="ptmr7t-x-x-70">size</span>                                                           </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-14-2"  
class="td11"> <!--l. 17--><p class="noindent" ><span 
class="ptmr7t-x-x-70">256</span>                                                                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-15-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-15-1"  
class="td11"> <!--l. 18--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Learning</span>
   <span 
class="ptmr7t-x-x-70">rate</span>
   <span 
class="ptmr7t-x-x-70">schedule</span>                                                    </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-15-2"  
class="td11"> <!--l. 18--><p class="noindent" ><span 
class="ptmr7t-x-x-70">cosine</span>
   <span 
class="ptmr7t-x-x-70">decay</span>
   <span 
class="ptmr7t-x-x-70">(</span><a 
href="#XLoshchilov2017SGDRSG"><span 
class="ptmr7t-x-x-70">Loshchilov</span>
   <span 
class="ptmr7t-x-x-70">&amp;</span>
   <span 
class="ptmr7t-x-x-70">Hutter</span></a><span 
class="ptmr7t-x-x-70">,</span><span 
class="ptmr7t-x-x-70">&#x00A0;</span><a 
href="#XLoshchilov2017SGDRSG"><span 
class="ptmr7t-x-x-70">2017</span></a><span 
class="ptmr7t-x-x-70">)</span>                                                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-16-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-16-1"  
class="td11"> <!--l. 19--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Warmup</span>
   <span 
class="ptmr7t-x-x-70">steps</span>
   <span 
class="ptmr7t-x-x-70">(</span><a 
href="#XGoyal2017AccurateLM"><span 
class="ptmr7t-x-x-70">Goyal</span>
   <span 
class="ptmr7t-x-x-70">et</span><span 
class="ptmr7t-x-x-70">&#x00A0;al.</span></a><span 
class="ptmr7t-x-x-70">,</span><span 
class="ptmr7t-x-x-70">&#x00A0;</span><a 
href="#XGoyal2017AccurateLM"><span 
class="ptmr7t-x-x-70">2017</span></a><span 
class="ptmr7t-x-x-70">)</span>                                            </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-16-2"  
class="td11"> <!--l. 19--><p class="noindent" ><span 
class="ptmr7t-x-x-70">10000</span>                                                                               </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0015-17-"><td  style="white-space:normal; text-align:left;" id="TBL-0015-17-1"  
class="td11"> <!--l. 20--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Training</span>
   <span 
class="ptmr7t-x-x-70">steps</span>                                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-0015-17-2"  
class="td11"> <!--l. 20--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1M</span>                                                                                    </td>
</tr></table></div>
<a 
 id="x1-2007"></a>
<br /> <div class="caption" 
><span class="id">Table 5: </span><span  
class="content">Configuration and training hyperparameters for VQGAN.</span></div><!--tex4ht:label?: x1-2006r5 -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
</div>
<span 
class="ptmb7t-">VQGAN Architecture</span>: Our VQGAN architecture is similar to the previous work (<a 
href="#Xesser2021taming">Esser et&#x00A0;al.</a>,&#x00A0;<a 
href="#Xesser2021taming">2021b</a>). It consists of
several residual blocks, downsample(encoder) and upsample (decoder) blocks. The main difference is that we remove
the non-local block to make the encoder and decoder fully convolutional to support different image sizes. In the
base VQGAN model, we apply 2 residual blocks in each resolution and the base channel dimension is 128. For the
finetuned decoder, we apply 4 residual blocks in each resolution and we also make the base channel dimension to be
256.
                                                                                             
                                                                                             
<a 
 id="x1-2008r13"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
 <table id="TBL-18" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-18-1g"><col 
id="TBL-18-1"><col 
id="TBL-18-2"><col 
id="TBL-18-3"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-18-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-18-1-1"  
class="td11">    <img 
src="figs/fine_tune_decoder/free6_ori.jpg" alt="PIC"  
width="146" height="146" >         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-18-1-2"  
class="td11">        <img 
src="figs/fine_tune_decoder/free6_reconstruced.jpg" alt="PIC"  
width="146" height="146" >                   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-18-1-3"  
class="td11">      <img 
src="figs/fine_tune_decoder/free6_ft_reconstruced.jpg" alt="PIC"  
width="146" height="146" >               </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-18-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-18-2-1"  
class="td11"> Input Image  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-18-2-2"  
class="td11"> VQGAN Reconstruction  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-18-2-3"  
class="td11"> Finetuned Decoder  </td></tr></table>
<a 
 id="x1-2009"></a>
 <div class="caption" 
><span class="id">Figure 13: </span><span  
class="content"><span 
class="ptmr7t-x-x-90">Visual example of the improvement from the fine-tuned decoder (</span><a 
href="#x1-20r5"><span 
class="ptmr7t-x-x-90">Section</span><span 
class="ptmr7t-x-x-90">&#x00A0;2.5</span></a><span 
class="ptmr7t-x-x-90">). Please zoom in by at least 200% to see the</span>
<span 
class="ptmr7t-x-x-90">difference between the VQGAN reconstruction and the reconstruction with a finetuned decoder. We can see especially that fine details</span>
<span 
class="ptmr7t-x-x-90">such as the house number (bottom left), the storefront sign (middle) and the bars on the windows (right) are better preserved in the</span>
<span 
class="ptmr7t-x-x-90">finetuned decoder.</span></span></div><!--tex4ht:label?: x1-2008r5 -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
<a 
 id="x1-2010r3"></a>
<span 
class="ptmb7t-">A.3.</span><span 
class="ptmb7t-">&#x00A0;</span><span 
class="ptmb7t-">Super</span>
    <span 
class="ptmb7t-">Resolution</span>
    <span 
class="ptmb7t-">Configurations</span>
<a 
 id="Q1-1-31"></a>
<div class="table">
                                                                                             
                                                                                             
<a 
 id="x1-2011r6"></a><hr class="float"><div class="float" 
>
                                                                                             
                                                                                             
<div class="tabular"> <table id="TBL-19" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-19-1g"><col 
id="TBL-19-1"></colgroup><colgroup id="TBL-19-2g"><col 
id="TBL-19-2"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-19-1-"><td  style="white-space:normal; text-align:left;" id="TBL-19-1-1"  
class="td11"> <!--l. 6--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Configuration</span>                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-19-1-2"  
class="td11"> <!--l. 6--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Value</span>                                                                                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-19-2-"><td  style="white-space:normal; text-align:left;" id="TBL-19-2-1"  
class="td11"> </td></tr><tr><td colspan="2"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-019-2-"><td  style="white-space:normal; text-align:left;" id="TBL-019-2-1"  
class="td11">
 <!--l. 7--><p class="noindent" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-019-3-"><td  style="white-space:normal; text-align:left;" id="TBL-019-3-1"  
class="td11"> <!--l. 7--><p class="noindent" >                           </td></tr><tr 
class="hline"><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-019-4-"><td  style="white-space:normal; text-align:left;" id="TBL-019-4-1"  
class="td11"> </td></tr><tr><td colspan="2"></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-4-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-4-1"  
class="td11">
 <!--l. 7--><p class="noindent" >                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-5-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-5-1"  
class="td11"> <!--l. 7--><p class="noindent" >                           </td></tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-6-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-6-1"  
class="td11"> <!--l. 8--><p class="noindent" ><span 
class="ptmr7t-x-x-70">LowRes</span>
   <span 
class="ptmr7t-x-x-70">Encoder</span>
   <span 
class="ptmr7t-x-x-70">Transformer</span>
   <span 
class="ptmr7t-x-x-70">Layers</span>                                                       </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-6-2"  
class="td11"> <!--l. 8--><p class="noindent" ><span 
class="ptmr7t-x-x-70">16</span>                                                                                     </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-7-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-7-1"  
class="td11"> <!--l. 9--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Number</span>
   <span 
class="ptmr7t-x-x-70">of</span>
   <span 
class="ptmr7t-x-x-70">Transformer</span>
   <span 
class="ptmr7t-x-x-70">layers</span>                                                        </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-7-2"  
class="td11"> <!--l. 9--><p class="noindent" ><span 
class="ptmr7t-x-x-70">32</span>                                                                                     </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-8-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-8-1"  
class="td11"> <!--l. 10--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Transformer</span>
   <span 
class="ptmr7t-x-x-70">Hidden</span>
   <span 
class="ptmr7t-x-x-70">Dimension</span>                                                </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-8-2"  
class="td11"> <!--l. 10--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1024</span>                                                                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-9-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-9-1"  
class="td11"> <!--l. 11--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Transformer</span>
   <span 
class="ptmr7t-x-x-70">MLP</span>
   <span 
class="ptmr7t-x-x-70">Dimension</span>                                                </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-9-2"  
class="td11"> <!--l. 11--><p class="noindent" ><span 
class="ptmr7t-x-x-70">4096</span>                                                                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-10-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-10-1"  
class="td11"> <!--l. 12--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Optimizer</span>                                                 </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-10-2"  
class="td11"> <!--l. 12--><p class="noindent" ><span 
class="ptmr7t-x-x-70">AdaFactor</span>
   <span 
class="ptmr7t-x-x-70">(</span><a 
href="#Xshazeer2018adafactor"><span 
class="ptmr7t-x-x-70">Shazeer</span>
   <span 
class="ptmr7t-x-x-70">&amp;</span>
   <span 
class="ptmr7t-x-x-70">Stern</span></a><span 
class="ptmr7t-x-x-70">,</span><span 
class="ptmr7t-x-x-70">&#x00A0;</span><a 
href="#Xshazeer2018adafactor"><span 
class="ptmr7t-x-x-70">2018</span></a><span 
class="ptmr7t-x-x-70">)</span>                                                                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-11-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-11-1"  
class="td11"> <!--l. 13--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Base</span>
   <span 
class="ptmr7t-x-x-70">learning</span>
   <span 
class="ptmr7t-x-x-70">rate</span>                                                            </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-11-2"  
class="td11"> <!--l. 13--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1e-4</span>                                                                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-12-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-12-1"  
class="td11"> <!--l. 14--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Weight</span>
   <span 
class="ptmr7t-x-x-70">decay</span>                                                        </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-12-2"  
class="td11"> <!--l. 14--><p class="noindent" ><span 
class="ptmr7t-x-x-70">0.045</span>                                                                                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-13-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-13-1"  
class="td11"> <!--l. 15--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Optimizer</span>
   <span 
class="ptmr7t-x-x-70">momentum</span>                                               </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-13-2"  
class="td11"> <!--l. 15--><p class="noindent" ><span 
class="cmmi-7">&#x03B2;</span><sub><span 
class="cmr-5">1</span></sub><span 
class="cmr-7">=</span><span 
class="cmr-7">0</span><span 
class="cmmi-7">.</span><span 
class="cmr-7">9</span><span 
class="cmmi-7">,&#x03B2;</span><sub><span 
class="cmr-5">2</span></sub><span 
class="cmr-7">=</span><span 
class="cmr-7">0</span><span 
class="cmmi-7">.</span><span 
class="cmr-7">96</span>                                    </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-14-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-14-1"  
class="td11"> <!--l. 16--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Batch</span>
   <span 
class="ptmr7t-x-x-70">size</span>                                                           </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-14-2"  
class="td11"> <!--l. 16--><p class="noindent" ><span 
class="ptmr7t-x-x-70">512</span>                                                                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-15-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-15-1"  
class="td11"> <!--l. 17--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Learning</span>
   <span 
class="ptmr7t-x-x-70">rate</span>
   <span 
class="ptmr7t-x-x-70">schedule</span>                                                    </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-15-2"  
class="td11"> <!--l. 17--><p class="noindent" ><span 
class="ptmr7t-x-x-70">cosine</span>
   <span 
class="ptmr7t-x-x-70">decay</span>
   <span 
class="ptmr7t-x-x-70">(</span><a 
href="#XLoshchilov2017SGDRSG"><span 
class="ptmr7t-x-x-70">Loshchilov</span>
   <span 
class="ptmr7t-x-x-70">&amp;</span>
   <span 
class="ptmr7t-x-x-70">Hutter</span></a><span 
class="ptmr7t-x-x-70">,</span><span 
class="ptmr7t-x-x-70">&#x00A0;</span><a 
href="#XLoshchilov2017SGDRSG"><span 
class="ptmr7t-x-x-70">2017</span></a><span 
class="ptmr7t-x-x-70">)</span>                                                                   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-16-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-16-1"  
class="td11"> <!--l. 18--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Warmup</span>
   <span 
class="ptmr7t-x-x-70">steps</span>                                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-16-2"  
class="td11"> <!--l. 18--><p class="noindent" ><span 
class="ptmr7t-x-x-70">5000</span>                                                                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-0019-17-"><td  style="white-space:normal; text-align:left;" id="TBL-0019-17-1"  
class="td11"> <!--l. 19--><p class="noindent" ><span 
class="ptmr7t-x-x-70">Training</span>
   <span 
class="ptmr7t-x-x-70">steps</span>                                                          </td><td  style="white-space:normal; text-align:left;" id="TBL-0019-17-2"  
class="td11"> <!--l. 19--><p class="noindent" ><span 
class="ptmr7t-x-x-70">1M</span>                                                                                    </td>
</tr></table></div>
<a 
 id="x1-2012"></a>
<br /> <div class="caption" 
><span class="id">Table 6: </span><span  
class="content">Configuration and training hyperparameters for the Super-Resolution Model.</span></div><!--tex4ht:label?: x1-2011r5 -->
                                                                                             
                                                                                             
</div><hr class="endfloat" />
</div>
 
</body></html> 

                                                                                             
                                                                                             
                                                                                             


